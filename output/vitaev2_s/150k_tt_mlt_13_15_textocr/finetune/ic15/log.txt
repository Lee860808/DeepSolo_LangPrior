[07/09 11:02:01] detectron2 INFO: Rank of current process: 0. World size: 1
[07/09 11:02:03] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                   1.26.4
detectron2              0.6 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.3
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.1 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version          470.256.02
CUDA_HOME               /usr/local/cuda
Pillow                  9.0.1
torchvision             0.11.2 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.10.0
----------------------  --------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/09 11:02:03] detectron2 INFO: Command line arguments: Namespace(config_file='configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth'])
[07/09 11:02:03] detectron2 INFO: Contents of args.config_file=configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  ViTAEv2:
    DROP_PATH_RATE: 0.2
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (800,900,1000,1100,1200,1300,1400)
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  IMS_PER_BATCH: 8
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-5
  WARMUP_ITERS: 0
  STEPS: (100000,)
  MAX_ITER: 1000
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 500
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15"
[07/09 11:02:03] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 800
  - 900
  - 1000
  - 1100
  - 1200
  - 1300
  - 1400
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_vitaev2_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth
OUTPUT_DIR: output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 1.0e-05
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_BACKBONE: 1.0e-05
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 100000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 500
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[07/09 11:02:03] detectron2 INFO: Full config saved to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[07/09 11:02:37] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ViTAEv2(
          (layers): ModuleList(
            (0): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(5, 5), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(8, 8), dilation=(3, 3))
                      (1): GELU()
                    )
                    (3): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(11, 11), dilation=(4, 4))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=256, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=64, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=64, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.015)
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
              )
            )
            (1): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16)
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), dilation=(3, 3))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=192, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=128, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=128, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
              )
            )
            (2): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (2): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.062)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (3): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.077)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (4): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.092)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (5): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.108)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (6): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (7): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
              )
            )
            (3): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[07/09 11:02:37] fvcore.common.checkpoint INFO: [Checkpointer] Loading from weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth ...
[07/09 11:02:39] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[07/09 11:02:39] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[07/09 11:02:39] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[07/09 11:02:39] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[07/09 11:02:39] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[07/09 11:02:39] d2.evaluation.evaluator INFO: Start inference on 500 batches
[07/09 11:02:54] d2.evaluation.evaluator INFO: Inference done 1/500. Dataloading: 0.9533 s/iter. Inference: 14.1695 s/iter. Eval: 0.0417 s/iter. Total: 15.1678 s/iter. ETA=2:06:08
[07/09 11:03:09] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0016 s/iter. Inference: 1.4671 s/iter. Eval: 0.0008 s/iter. Total: 1.4695 s/iter. ETA=0:11:58
[07/09 11:03:14] d2.evaluation.evaluator INFO: Inference done 15/500. Dataloading: 0.0017 s/iter. Inference: 1.4629 s/iter. Eval: 0.0009 s/iter. Total: 1.4655 s/iter. ETA=0:11:50
[07/09 11:03:20] d2.evaluation.evaluator INFO: Inference done 19/500. Dataloading: 0.0018 s/iter. Inference: 1.4632 s/iter. Eval: 0.0009 s/iter. Total: 1.4659 s/iter. ETA=0:11:45
[07/09 11:03:26] d2.evaluation.evaluator INFO: Inference done 23/500. Dataloading: 0.0018 s/iter. Inference: 1.4625 s/iter. Eval: 0.0010 s/iter. Total: 1.4654 s/iter. ETA=0:11:38
[07/09 11:03:32] d2.evaluation.evaluator INFO: Inference done 27/500. Dataloading: 0.0019 s/iter. Inference: 1.4618 s/iter. Eval: 0.0009 s/iter. Total: 1.4646 s/iter. ETA=0:11:32
[07/09 11:03:38] d2.evaluation.evaluator INFO: Inference done 31/500. Dataloading: 0.0019 s/iter. Inference: 1.4609 s/iter. Eval: 0.0009 s/iter. Total: 1.4638 s/iter. ETA=0:11:26
[07/09 11:03:44] d2.evaluation.evaluator INFO: Inference done 35/500. Dataloading: 0.0019 s/iter. Inference: 1.4617 s/iter. Eval: 0.0009 s/iter. Total: 1.4646 s/iter. ETA=0:11:21
[07/09 11:03:50] d2.evaluation.evaluator INFO: Inference done 39/500. Dataloading: 0.0019 s/iter. Inference: 1.4675 s/iter. Eval: 0.0009 s/iter. Total: 1.4703 s/iter. ETA=0:11:17
[07/09 11:03:56] d2.evaluation.evaluator INFO: Inference done 43/500. Dataloading: 0.0020 s/iter. Inference: 1.4684 s/iter. Eval: 0.0008 s/iter. Total: 1.4712 s/iter. ETA=0:11:12
[07/09 11:04:02] d2.evaluation.evaluator INFO: Inference done 47/500. Dataloading: 0.0020 s/iter. Inference: 1.4707 s/iter. Eval: 0.0008 s/iter. Total: 1.4736 s/iter. ETA=0:11:07
[07/09 11:04:08] d2.evaluation.evaluator INFO: Inference done 51/500. Dataloading: 0.0020 s/iter. Inference: 1.4712 s/iter. Eval: 0.0009 s/iter. Total: 1.4741 s/iter. ETA=0:11:01
[07/09 11:04:14] d2.evaluation.evaluator INFO: Inference done 55/500. Dataloading: 0.0020 s/iter. Inference: 1.4739 s/iter. Eval: 0.0008 s/iter. Total: 1.4768 s/iter. ETA=0:10:57
[07/09 11:04:20] d2.evaluation.evaluator INFO: Inference done 59/500. Dataloading: 0.0020 s/iter. Inference: 1.4762 s/iter. Eval: 0.0008 s/iter. Total: 1.4790 s/iter. ETA=0:10:52
[07/09 11:04:26] d2.evaluation.evaluator INFO: Inference done 63/500. Dataloading: 0.0020 s/iter. Inference: 1.4756 s/iter. Eval: 0.0008 s/iter. Total: 1.4785 s/iter. ETA=0:10:46
[07/09 11:04:31] d2.evaluation.evaluator INFO: Inference done 67/500. Dataloading: 0.0020 s/iter. Inference: 1.4751 s/iter. Eval: 0.0008 s/iter. Total: 1.4780 s/iter. ETA=0:10:39
[07/09 11:04:37] d2.evaluation.evaluator INFO: Inference done 71/500. Dataloading: 0.0020 s/iter. Inference: 1.4759 s/iter. Eval: 0.0008 s/iter. Total: 1.4789 s/iter. ETA=0:10:34
[07/09 11:04:43] d2.evaluation.evaluator INFO: Inference done 75/500. Dataloading: 0.0020 s/iter. Inference: 1.4751 s/iter. Eval: 0.0009 s/iter. Total: 1.4780 s/iter. ETA=0:10:28
[07/09 11:04:49] d2.evaluation.evaluator INFO: Inference done 79/500. Dataloading: 0.0020 s/iter. Inference: 1.4748 s/iter. Eval: 0.0009 s/iter. Total: 1.4778 s/iter. ETA=0:10:22
[07/09 11:04:55] d2.evaluation.evaluator INFO: Inference done 83/500. Dataloading: 0.0020 s/iter. Inference: 1.4750 s/iter. Eval: 0.0009 s/iter. Total: 1.4780 s/iter. ETA=0:10:16
[07/09 11:05:01] d2.evaluation.evaluator INFO: Inference done 87/500. Dataloading: 0.0021 s/iter. Inference: 1.4754 s/iter. Eval: 0.0009 s/iter. Total: 1.4784 s/iter. ETA=0:10:10
[07/09 11:05:07] d2.evaluation.evaluator INFO: Inference done 91/500. Dataloading: 0.0021 s/iter. Inference: 1.4750 s/iter. Eval: 0.0009 s/iter. Total: 1.4780 s/iter. ETA=0:10:04
[07/09 11:05:13] d2.evaluation.evaluator INFO: Inference done 95/500. Dataloading: 0.0021 s/iter. Inference: 1.4749 s/iter. Eval: 0.0009 s/iter. Total: 1.4779 s/iter. ETA=0:09:58
[07/09 11:05:19] d2.evaluation.evaluator INFO: Inference done 99/500. Dataloading: 0.0021 s/iter. Inference: 1.4747 s/iter. Eval: 0.0009 s/iter. Total: 1.4777 s/iter. ETA=0:09:52
[07/09 11:05:25] d2.evaluation.evaluator INFO: Inference done 103/500. Dataloading: 0.0021 s/iter. Inference: 1.4745 s/iter. Eval: 0.0009 s/iter. Total: 1.4775 s/iter. ETA=0:09:46
[07/09 11:05:31] d2.evaluation.evaluator INFO: Inference done 107/500. Dataloading: 0.0021 s/iter. Inference: 1.4748 s/iter. Eval: 0.0009 s/iter. Total: 1.4778 s/iter. ETA=0:09:40
[07/09 11:05:36] d2.evaluation.evaluator INFO: Inference done 111/500. Dataloading: 0.0021 s/iter. Inference: 1.4750 s/iter. Eval: 0.0009 s/iter. Total: 1.4780 s/iter. ETA=0:09:34
[07/09 11:05:42] d2.evaluation.evaluator INFO: Inference done 115/500. Dataloading: 0.0021 s/iter. Inference: 1.4744 s/iter. Eval: 0.0009 s/iter. Total: 1.4774 s/iter. ETA=0:09:28
[07/09 11:05:48] d2.evaluation.evaluator INFO: Inference done 119/500. Dataloading: 0.0021 s/iter. Inference: 1.4737 s/iter. Eval: 0.0009 s/iter. Total: 1.4767 s/iter. ETA=0:09:22
[07/09 11:05:54] d2.evaluation.evaluator INFO: Inference done 123/500. Dataloading: 0.0021 s/iter. Inference: 1.4732 s/iter. Eval: 0.0009 s/iter. Total: 1.4763 s/iter. ETA=0:09:16
[07/09 11:06:00] d2.evaluation.evaluator INFO: Inference done 127/500. Dataloading: 0.0021 s/iter. Inference: 1.4727 s/iter. Eval: 0.0009 s/iter. Total: 1.4757 s/iter. ETA=0:09:10
[07/09 11:06:06] d2.evaluation.evaluator INFO: Inference done 131/500. Dataloading: 0.0021 s/iter. Inference: 1.4724 s/iter. Eval: 0.0009 s/iter. Total: 1.4754 s/iter. ETA=0:09:04
[07/09 11:06:12] d2.evaluation.evaluator INFO: Inference done 135/500. Dataloading: 0.0021 s/iter. Inference: 1.4721 s/iter. Eval: 0.0009 s/iter. Total: 1.4751 s/iter. ETA=0:08:58
[07/09 11:06:17] d2.evaluation.evaluator INFO: Inference done 139/500. Dataloading: 0.0021 s/iter. Inference: 1.4721 s/iter. Eval: 0.0009 s/iter. Total: 1.4751 s/iter. ETA=0:08:52
[07/09 11:06:23] d2.evaluation.evaluator INFO: Inference done 143/500. Dataloading: 0.0021 s/iter. Inference: 1.4718 s/iter. Eval: 0.0009 s/iter. Total: 1.4748 s/iter. ETA=0:08:46
[07/09 11:06:29] d2.evaluation.evaluator INFO: Inference done 147/500. Dataloading: 0.0021 s/iter. Inference: 1.4717 s/iter. Eval: 0.0009 s/iter. Total: 1.4748 s/iter. ETA=0:08:40
[07/09 11:06:35] d2.evaluation.evaluator INFO: Inference done 151/500. Dataloading: 0.0021 s/iter. Inference: 1.4718 s/iter. Eval: 0.0009 s/iter. Total: 1.4748 s/iter. ETA=0:08:34
[07/09 11:06:41] d2.evaluation.evaluator INFO: Inference done 155/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0009 s/iter. Total: 1.4749 s/iter. ETA=0:08:28
[07/09 11:06:47] d2.evaluation.evaluator INFO: Inference done 159/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0009 s/iter. Total: 1.4749 s/iter. ETA=0:08:22
[07/09 11:06:53] d2.evaluation.evaluator INFO: Inference done 163/500. Dataloading: 0.0021 s/iter. Inference: 1.4720 s/iter. Eval: 0.0008 s/iter. Total: 1.4750 s/iter. ETA=0:08:17
[07/09 11:06:59] d2.evaluation.evaluator INFO: Inference done 167/500. Dataloading: 0.0021 s/iter. Inference: 1.4720 s/iter. Eval: 0.0009 s/iter. Total: 1.4750 s/iter. ETA=0:08:11
[07/09 11:07:05] d2.evaluation.evaluator INFO: Inference done 171/500. Dataloading: 0.0021 s/iter. Inference: 1.4720 s/iter. Eval: 0.0008 s/iter. Total: 1.4750 s/iter. ETA=0:08:05
[07/09 11:07:11] d2.evaluation.evaluator INFO: Inference done 175/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0009 s/iter. Total: 1.4749 s/iter. ETA=0:07:59
[07/09 11:07:16] d2.evaluation.evaluator INFO: Inference done 179/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0008 s/iter. Total: 1.4749 s/iter. ETA=0:07:53
[07/09 11:07:22] d2.evaluation.evaluator INFO: Inference done 183/500. Dataloading: 0.0021 s/iter. Inference: 1.4718 s/iter. Eval: 0.0008 s/iter. Total: 1.4748 s/iter. ETA=0:07:47
[07/09 11:07:28] d2.evaluation.evaluator INFO: Inference done 187/500. Dataloading: 0.0021 s/iter. Inference: 1.4717 s/iter. Eval: 0.0008 s/iter. Total: 1.4747 s/iter. ETA=0:07:41
[07/09 11:07:34] d2.evaluation.evaluator INFO: Inference done 191/500. Dataloading: 0.0021 s/iter. Inference: 1.4714 s/iter. Eval: 0.0008 s/iter. Total: 1.4744 s/iter. ETA=0:07:35
[07/09 11:07:40] d2.evaluation.evaluator INFO: Inference done 195/500. Dataloading: 0.0021 s/iter. Inference: 1.4712 s/iter. Eval: 0.0008 s/iter. Total: 1.4742 s/iter. ETA=0:07:29
[07/09 11:07:46] d2.evaluation.evaluator INFO: Inference done 199/500. Dataloading: 0.0021 s/iter. Inference: 1.4710 s/iter. Eval: 0.0008 s/iter. Total: 1.4740 s/iter. ETA=0:07:23
[07/09 11:07:52] d2.evaluation.evaluator INFO: Inference done 203/500. Dataloading: 0.0021 s/iter. Inference: 1.4708 s/iter. Eval: 0.0008 s/iter. Total: 1.4738 s/iter. ETA=0:07:17
[07/09 11:07:57] d2.evaluation.evaluator INFO: Inference done 207/500. Dataloading: 0.0021 s/iter. Inference: 1.4706 s/iter. Eval: 0.0008 s/iter. Total: 1.4736 s/iter. ETA=0:07:11
[07/09 11:08:03] d2.evaluation.evaluator INFO: Inference done 211/500. Dataloading: 0.0021 s/iter. Inference: 1.4705 s/iter. Eval: 0.0008 s/iter. Total: 1.4735 s/iter. ETA=0:07:05
[07/09 11:08:09] d2.evaluation.evaluator INFO: Inference done 215/500. Dataloading: 0.0021 s/iter. Inference: 1.4705 s/iter. Eval: 0.0008 s/iter. Total: 1.4735 s/iter. ETA=0:06:59
[07/09 11:08:15] d2.evaluation.evaluator INFO: Inference done 219/500. Dataloading: 0.0021 s/iter. Inference: 1.4703 s/iter. Eval: 0.0008 s/iter. Total: 1.4733 s/iter. ETA=0:06:54
[07/09 11:08:21] d2.evaluation.evaluator INFO: Inference done 223/500. Dataloading: 0.0021 s/iter. Inference: 1.4701 s/iter. Eval: 0.0008 s/iter. Total: 1.4731 s/iter. ETA=0:06:48
[07/09 11:08:27] d2.evaluation.evaluator INFO: Inference done 227/500. Dataloading: 0.0021 s/iter. Inference: 1.4699 s/iter. Eval: 0.0008 s/iter. Total: 1.4729 s/iter. ETA=0:06:42
[07/09 11:08:33] d2.evaluation.evaluator INFO: Inference done 231/500. Dataloading: 0.0021 s/iter. Inference: 1.4698 s/iter. Eval: 0.0008 s/iter. Total: 1.4728 s/iter. ETA=0:06:36
[07/09 11:08:38] d2.evaluation.evaluator INFO: Inference done 235/500. Dataloading: 0.0021 s/iter. Inference: 1.4697 s/iter. Eval: 0.0008 s/iter. Total: 1.4727 s/iter. ETA=0:06:30
[07/09 11:08:44] d2.evaluation.evaluator INFO: Inference done 239/500. Dataloading: 0.0021 s/iter. Inference: 1.4695 s/iter. Eval: 0.0008 s/iter. Total: 1.4725 s/iter. ETA=0:06:24
[07/09 11:08:50] d2.evaluation.evaluator INFO: Inference done 243/500. Dataloading: 0.0021 s/iter. Inference: 1.4693 s/iter. Eval: 0.0008 s/iter. Total: 1.4723 s/iter. ETA=0:06:18
[07/09 11:08:56] d2.evaluation.evaluator INFO: Inference done 247/500. Dataloading: 0.0021 s/iter. Inference: 1.4692 s/iter. Eval: 0.0008 s/iter. Total: 1.4722 s/iter. ETA=0:06:12
[07/09 11:09:02] d2.evaluation.evaluator INFO: Inference done 251/500. Dataloading: 0.0021 s/iter. Inference: 1.4691 s/iter. Eval: 0.0008 s/iter. Total: 1.4721 s/iter. ETA=0:06:06
[07/09 11:09:08] d2.evaluation.evaluator INFO: Inference done 255/500. Dataloading: 0.0021 s/iter. Inference: 1.4690 s/iter. Eval: 0.0008 s/iter. Total: 1.4719 s/iter. ETA=0:06:00
[07/09 11:09:14] d2.evaluation.evaluator INFO: Inference done 259/500. Dataloading: 0.0021 s/iter. Inference: 1.4688 s/iter. Eval: 0.0008 s/iter. Total: 1.4718 s/iter. ETA=0:05:54
[07/09 11:09:19] d2.evaluation.evaluator INFO: Inference done 263/500. Dataloading: 0.0021 s/iter. Inference: 1.4687 s/iter. Eval: 0.0008 s/iter. Total: 1.4717 s/iter. ETA=0:05:48
[07/09 11:09:25] d2.evaluation.evaluator INFO: Inference done 267/500. Dataloading: 0.0021 s/iter. Inference: 1.4686 s/iter. Eval: 0.0008 s/iter. Total: 1.4716 s/iter. ETA=0:05:42
[07/09 11:09:31] d2.evaluation.evaluator INFO: Inference done 271/500. Dataloading: 0.0021 s/iter. Inference: 1.4685 s/iter. Eval: 0.0008 s/iter. Total: 1.4715 s/iter. ETA=0:05:36
[07/09 11:09:37] d2.evaluation.evaluator INFO: Inference done 275/500. Dataloading: 0.0021 s/iter. Inference: 1.4684 s/iter. Eval: 0.0008 s/iter. Total: 1.4714 s/iter. ETA=0:05:31
[07/09 11:09:43] d2.evaluation.evaluator INFO: Inference done 279/500. Dataloading: 0.0021 s/iter. Inference: 1.4683 s/iter. Eval: 0.0008 s/iter. Total: 1.4713 s/iter. ETA=0:05:25
[07/09 11:09:49] d2.evaluation.evaluator INFO: Inference done 283/500. Dataloading: 0.0021 s/iter. Inference: 1.4682 s/iter. Eval: 0.0008 s/iter. Total: 1.4712 s/iter. ETA=0:05:19
[07/09 11:09:55] d2.evaluation.evaluator INFO: Inference done 287/500. Dataloading: 0.0021 s/iter. Inference: 1.4681 s/iter. Eval: 0.0008 s/iter. Total: 1.4711 s/iter. ETA=0:05:13
[07/09 11:10:01] d2.evaluation.evaluator INFO: Inference done 291/500. Dataloading: 0.0021 s/iter. Inference: 1.4681 s/iter. Eval: 0.0008 s/iter. Total: 1.4710 s/iter. ETA=0:05:07
[07/09 11:10:06] d2.evaluation.evaluator INFO: Inference done 295/500. Dataloading: 0.0021 s/iter. Inference: 1.4680 s/iter. Eval: 0.0008 s/iter. Total: 1.4710 s/iter. ETA=0:05:01
[07/09 11:10:12] d2.evaluation.evaluator INFO: Inference done 299/500. Dataloading: 0.0021 s/iter. Inference: 1.4681 s/iter. Eval: 0.0008 s/iter. Total: 1.4710 s/iter. ETA=0:04:55
[07/09 11:10:18] d2.evaluation.evaluator INFO: Inference done 303/500. Dataloading: 0.0021 s/iter. Inference: 1.4679 s/iter. Eval: 0.0008 s/iter. Total: 1.4709 s/iter. ETA=0:04:49
[07/09 11:10:24] d2.evaluation.evaluator INFO: Inference done 307/500. Dataloading: 0.0021 s/iter. Inference: 1.4679 s/iter. Eval: 0.0008 s/iter. Total: 1.4708 s/iter. ETA=0:04:43
[07/09 11:10:30] d2.evaluation.evaluator INFO: Inference done 311/500. Dataloading: 0.0021 s/iter. Inference: 1.4678 s/iter. Eval: 0.0008 s/iter. Total: 1.4707 s/iter. ETA=0:04:37
[07/09 11:10:36] d2.evaluation.evaluator INFO: Inference done 315/500. Dataloading: 0.0021 s/iter. Inference: 1.4677 s/iter. Eval: 0.0008 s/iter. Total: 1.4707 s/iter. ETA=0:04:32
[07/09 11:10:42] d2.evaluation.evaluator INFO: Inference done 319/500. Dataloading: 0.0021 s/iter. Inference: 1.4676 s/iter. Eval: 0.0008 s/iter. Total: 1.4706 s/iter. ETA=0:04:26
[07/09 11:10:47] d2.evaluation.evaluator INFO: Inference done 323/500. Dataloading: 0.0021 s/iter. Inference: 1.4676 s/iter. Eval: 0.0008 s/iter. Total: 1.4706 s/iter. ETA=0:04:20
[07/09 11:10:53] d2.evaluation.evaluator INFO: Inference done 327/500. Dataloading: 0.0021 s/iter. Inference: 1.4675 s/iter. Eval: 0.0008 s/iter. Total: 1.4705 s/iter. ETA=0:04:14
[07/09 11:10:59] d2.evaluation.evaluator INFO: Inference done 331/500. Dataloading: 0.0021 s/iter. Inference: 1.4675 s/iter. Eval: 0.0008 s/iter. Total: 1.4704 s/iter. ETA=0:04:08
[07/09 11:11:05] d2.evaluation.evaluator INFO: Inference done 335/500. Dataloading: 0.0021 s/iter. Inference: 1.4674 s/iter. Eval: 0.0008 s/iter. Total: 1.4704 s/iter. ETA=0:04:02
[07/09 11:11:11] d2.evaluation.evaluator INFO: Inference done 339/500. Dataloading: 0.0021 s/iter. Inference: 1.4673 s/iter. Eval: 0.0008 s/iter. Total: 1.4703 s/iter. ETA=0:03:56
[07/09 11:11:17] d2.evaluation.evaluator INFO: Inference done 343/500. Dataloading: 0.0021 s/iter. Inference: 1.4673 s/iter. Eval: 0.0008 s/iter. Total: 1.4703 s/iter. ETA=0:03:50
[07/09 11:11:23] d2.evaluation.evaluator INFO: Inference done 347/500. Dataloading: 0.0021 s/iter. Inference: 1.4672 s/iter. Eval: 0.0008 s/iter. Total: 1.4702 s/iter. ETA=0:03:44
[07/09 11:11:28] d2.evaluation.evaluator INFO: Inference done 351/500. Dataloading: 0.0021 s/iter. Inference: 1.4671 s/iter. Eval: 0.0008 s/iter. Total: 1.4701 s/iter. ETA=0:03:39
[07/09 11:11:35] d2.evaluation.evaluator INFO: Inference done 355/500. Dataloading: 0.0021 s/iter. Inference: 1.4677 s/iter. Eval: 0.0008 s/iter. Total: 1.4707 s/iter. ETA=0:03:33
[07/09 11:11:41] d2.evaluation.evaluator INFO: Inference done 359/500. Dataloading: 0.0021 s/iter. Inference: 1.4686 s/iter. Eval: 0.0008 s/iter. Total: 1.4715 s/iter. ETA=0:03:27
[07/09 11:11:47] d2.evaluation.evaluator INFO: Inference done 363/500. Dataloading: 0.0021 s/iter. Inference: 1.4690 s/iter. Eval: 0.0008 s/iter. Total: 1.4720 s/iter. ETA=0:03:21
[07/09 11:11:53] d2.evaluation.evaluator INFO: Inference done 367/500. Dataloading: 0.0021 s/iter. Inference: 1.4699 s/iter. Eval: 0.0008 s/iter. Total: 1.4729 s/iter. ETA=0:03:15
[07/09 11:11:59] d2.evaluation.evaluator INFO: Inference done 371/500. Dataloading: 0.0021 s/iter. Inference: 1.4701 s/iter. Eval: 0.0008 s/iter. Total: 1.4731 s/iter. ETA=0:03:10
[07/09 11:12:05] d2.evaluation.evaluator INFO: Inference done 375/500. Dataloading: 0.0021 s/iter. Inference: 1.4711 s/iter. Eval: 0.0008 s/iter. Total: 1.4741 s/iter. ETA=0:03:04
[07/09 11:12:11] d2.evaluation.evaluator INFO: Inference done 379/500. Dataloading: 0.0021 s/iter. Inference: 1.4716 s/iter. Eval: 0.0008 s/iter. Total: 1.4746 s/iter. ETA=0:02:58
[07/09 11:12:17] d2.evaluation.evaluator INFO: Inference done 383/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0008 s/iter. Total: 1.4749 s/iter. ETA=0:02:52
[07/09 11:12:23] d2.evaluation.evaluator INFO: Inference done 387/500. Dataloading: 0.0021 s/iter. Inference: 1.4719 s/iter. Eval: 0.0008 s/iter. Total: 1.4749 s/iter. ETA=0:02:46
[07/09 11:12:30] d2.evaluation.evaluator INFO: Inference done 391/500. Dataloading: 0.0021 s/iter. Inference: 1.4730 s/iter. Eval: 0.0008 s/iter. Total: 1.4760 s/iter. ETA=0:02:40
[07/09 11:12:36] d2.evaluation.evaluator INFO: Inference done 395/500. Dataloading: 0.0021 s/iter. Inference: 1.4733 s/iter. Eval: 0.0008 s/iter. Total: 1.4763 s/iter. ETA=0:02:35
[07/09 11:12:41] d2.evaluation.evaluator INFO: Inference done 399/500. Dataloading: 0.0021 s/iter. Inference: 1.4732 s/iter. Eval: 0.0008 s/iter. Total: 1.4762 s/iter. ETA=0:02:29
[07/09 11:12:47] d2.evaluation.evaluator INFO: Inference done 403/500. Dataloading: 0.0021 s/iter. Inference: 1.4731 s/iter. Eval: 0.0008 s/iter. Total: 1.4761 s/iter. ETA=0:02:23
[07/09 11:12:53] d2.evaluation.evaluator INFO: Inference done 407/500. Dataloading: 0.0021 s/iter. Inference: 1.4730 s/iter. Eval: 0.0008 s/iter. Total: 1.4760 s/iter. ETA=0:02:17
[07/09 11:12:59] d2.evaluation.evaluator INFO: Inference done 411/500. Dataloading: 0.0021 s/iter. Inference: 1.4729 s/iter. Eval: 0.0008 s/iter. Total: 1.4759 s/iter. ETA=0:02:11
[07/09 11:13:05] d2.evaluation.evaluator INFO: Inference done 415/500. Dataloading: 0.0021 s/iter. Inference: 1.4727 s/iter. Eval: 0.0008 s/iter. Total: 1.4757 s/iter. ETA=0:02:05
[07/09 11:13:11] d2.evaluation.evaluator INFO: Inference done 419/500. Dataloading: 0.0021 s/iter. Inference: 1.4727 s/iter. Eval: 0.0008 s/iter. Total: 1.4756 s/iter. ETA=0:01:59
[07/09 11:13:17] d2.evaluation.evaluator INFO: Inference done 423/500. Dataloading: 0.0021 s/iter. Inference: 1.4725 s/iter. Eval: 0.0008 s/iter. Total: 1.4755 s/iter. ETA=0:01:53
[07/09 11:13:22] d2.evaluation.evaluator INFO: Inference done 427/500. Dataloading: 0.0021 s/iter. Inference: 1.4724 s/iter. Eval: 0.0008 s/iter. Total: 1.4754 s/iter. ETA=0:01:47
[07/09 11:13:28] d2.evaluation.evaluator INFO: Inference done 431/500. Dataloading: 0.0021 s/iter. Inference: 1.4723 s/iter. Eval: 0.0008 s/iter. Total: 1.4753 s/iter. ETA=0:01:41
[07/09 11:13:34] d2.evaluation.evaluator INFO: Inference done 435/500. Dataloading: 0.0022 s/iter. Inference: 1.4722 s/iter. Eval: 0.0008 s/iter. Total: 1.4752 s/iter. ETA=0:01:35
[07/09 11:13:40] d2.evaluation.evaluator INFO: Inference done 439/500. Dataloading: 0.0022 s/iter. Inference: 1.4721 s/iter. Eval: 0.0008 s/iter. Total: 1.4751 s/iter. ETA=0:01:29
[07/09 11:13:46] d2.evaluation.evaluator INFO: Inference done 443/500. Dataloading: 0.0022 s/iter. Inference: 1.4720 s/iter. Eval: 0.0008 s/iter. Total: 1.4750 s/iter. ETA=0:01:24
[07/09 11:13:52] d2.evaluation.evaluator INFO: Inference done 447/500. Dataloading: 0.0022 s/iter. Inference: 1.4719 s/iter. Eval: 0.0008 s/iter. Total: 1.4749 s/iter. ETA=0:01:18
[07/09 11:13:58] d2.evaluation.evaluator INFO: Inference done 451/500. Dataloading: 0.0022 s/iter. Inference: 1.4718 s/iter. Eval: 0.0008 s/iter. Total: 1.4748 s/iter. ETA=0:01:12
[07/09 11:14:03] d2.evaluation.evaluator INFO: Inference done 455/500. Dataloading: 0.0022 s/iter. Inference: 1.4717 s/iter. Eval: 0.0008 s/iter. Total: 1.4747 s/iter. ETA=0:01:06
[07/09 11:14:09] d2.evaluation.evaluator INFO: Inference done 459/500. Dataloading: 0.0022 s/iter. Inference: 1.4716 s/iter. Eval: 0.0008 s/iter. Total: 1.4746 s/iter. ETA=0:01:00
[07/09 11:14:15] d2.evaluation.evaluator INFO: Inference done 463/500. Dataloading: 0.0022 s/iter. Inference: 1.4715 s/iter. Eval: 0.0008 s/iter. Total: 1.4745 s/iter. ETA=0:00:54
[07/09 11:14:21] d2.evaluation.evaluator INFO: Inference done 467/500. Dataloading: 0.0022 s/iter. Inference: 1.4714 s/iter. Eval: 0.0008 s/iter. Total: 1.4744 s/iter. ETA=0:00:48
[07/09 11:14:27] d2.evaluation.evaluator INFO: Inference done 471/500. Dataloading: 0.0022 s/iter. Inference: 1.4713 s/iter. Eval: 0.0008 s/iter. Total: 1.4743 s/iter. ETA=0:00:42
[07/09 11:14:33] d2.evaluation.evaluator INFO: Inference done 475/500. Dataloading: 0.0022 s/iter. Inference: 1.4712 s/iter. Eval: 0.0008 s/iter. Total: 1.4742 s/iter. ETA=0:00:36
[07/09 11:14:39] d2.evaluation.evaluator INFO: Inference done 479/500. Dataloading: 0.0022 s/iter. Inference: 1.4711 s/iter. Eval: 0.0008 s/iter. Total: 1.4741 s/iter. ETA=0:00:30
[07/09 11:14:44] d2.evaluation.evaluator INFO: Inference done 483/500. Dataloading: 0.0022 s/iter. Inference: 1.4710 s/iter. Eval: 0.0008 s/iter. Total: 1.4740 s/iter. ETA=0:00:25
[07/09 11:14:50] d2.evaluation.evaluator INFO: Inference done 487/500. Dataloading: 0.0022 s/iter. Inference: 1.4709 s/iter. Eval: 0.0008 s/iter. Total: 1.4740 s/iter. ETA=0:00:19
[07/09 11:14:56] d2.evaluation.evaluator INFO: Inference done 491/500. Dataloading: 0.0022 s/iter. Inference: 1.4708 s/iter. Eval: 0.0008 s/iter. Total: 1.4738 s/iter. ETA=0:00:13
[07/09 11:15:02] d2.evaluation.evaluator INFO: Inference done 495/500. Dataloading: 0.0022 s/iter. Inference: 1.4707 s/iter. Eval: 0.0008 s/iter. Total: 1.4737 s/iter. ETA=0:00:07
[07/09 11:15:08] d2.evaluation.evaluator INFO: Inference done 499/500. Dataloading: 0.0022 s/iter. Inference: 1.4706 s/iter. Eval: 0.0008 s/iter. Total: 1.4736 s/iter. ETA=0:00:01
[07/09 11:15:09] d2.evaluation.evaluator INFO: Total inference time: 0:12:09.527549 (1.473793 s / iter per device, on 1 devices)
[07/09 11:15:09] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:12:07 (1.470571 s / iter per device, on 1 devices)
[07/09 11:15:09] adet.evaluation.text_evaluation_all INFO: Saving results to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[07/09 11:15:12] d2.engine.defaults INFO: Evaluation results for ic15_test in csv format:
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: Task: DETECTION_ONLY_RESULTS
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: 0.6115,0.7814,0.6861
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: Task: None-E2E_RESULTS
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: 0.4706,0.6013,0.5280
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: Task: Strong-E2E_RESULTS
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 11:15:12] d2.evaluation.testing INFO: copypaste: 0.7674,0.7116,0.7384
[07/09 19:05:09] detectron2 INFO: Rank of current process: 0. World size: 1
[07/09 19:05:10] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                   1.26.4
detectron2              0.6 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.3
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.1 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version          470.256.02
CUDA_HOME               /usr/local/cuda
Pillow                  9.0.1
torchvision             0.11.2 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.10.0
----------------------  --------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/09 19:05:10] detectron2 INFO: Command line arguments: Namespace(config_file='configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'weights/ic15_vitaev2-s_finetune_synth-tt-mlt-13-15-textocr.pth'])
[07/09 19:05:10] detectron2 INFO: Contents of args.config_file=configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  ViTAEv2:
    DROP_PATH_RATE: 0.2
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (800,900,1000,1100,1200,1300,1400)
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  IMS_PER_BATCH: 8
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-5
  WARMUP_ITERS: 0
  STEPS: (100000,)
  MAX_ITER: 1000
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 500
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15"
[07/09 19:05:10] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 800
  - 900
  - 1000
  - 1100
  - 1200
  - 1300
  - 1400
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_vitaev2_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: weights/ic15_vitaev2-s_finetune_synth-tt-mlt-13-15-textocr.pth
OUTPUT_DIR: output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 1.0e-05
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_BACKBONE: 1.0e-05
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 100000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 500
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[07/09 19:05:10] detectron2 INFO: Full config saved to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[07/09 19:05:16] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ViTAEv2(
          (layers): ModuleList(
            (0): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(5, 5), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(8, 8), dilation=(3, 3))
                      (1): GELU()
                    )
                    (3): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(11, 11), dilation=(4, 4))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=256, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=64, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=64, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.015)
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
              )
            )
            (1): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16)
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), dilation=(3, 3))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=192, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=128, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=128, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
              )
            )
            (2): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (2): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.062)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (3): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.077)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (4): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.092)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (5): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.108)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (6): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (7): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
              )
            )
            (3): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[07/09 19:05:16] fvcore.common.checkpoint INFO: [Checkpointer] Loading from weights/ic15_vitaev2-s_finetune_synth-tt-mlt-13-15-textocr.pth ...
[07/09 19:05:18] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[07/09 19:05:18] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[07/09 19:05:18] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[07/09 19:05:18] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[07/09 19:05:18] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[07/09 19:05:18] d2.evaluation.evaluator INFO: Start inference on 500 batches
[07/09 19:05:37] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0017 s/iter. Inference: 1.5141 s/iter. Eval: 0.0005 s/iter. Total: 1.5164 s/iter. ETA=0:12:21
[07/09 19:05:43] d2.evaluation.evaluator INFO: Inference done 15/500. Dataloading: 0.0017 s/iter. Inference: 1.5114 s/iter. Eval: 0.0006 s/iter. Total: 1.5137 s/iter. ETA=0:12:14
[07/09 19:05:49] d2.evaluation.evaluator INFO: Inference done 19/500. Dataloading: 0.0017 s/iter. Inference: 1.5112 s/iter. Eval: 0.0006 s/iter. Total: 1.5136 s/iter. ETA=0:12:08
[07/09 19:05:55] d2.evaluation.evaluator INFO: Inference done 23/500. Dataloading: 0.0018 s/iter. Inference: 1.5125 s/iter. Eval: 0.0006 s/iter. Total: 1.5151 s/iter. ETA=0:12:02
[07/09 19:06:01] d2.evaluation.evaluator INFO: Inference done 27/500. Dataloading: 0.0019 s/iter. Inference: 1.5137 s/iter. Eval: 0.0006 s/iter. Total: 1.5163 s/iter. ETA=0:11:57
[07/09 19:06:07] d2.evaluation.evaluator INFO: Inference done 31/500. Dataloading: 0.0019 s/iter. Inference: 1.5156 s/iter. Eval: 0.0007 s/iter. Total: 1.5183 s/iter. ETA=0:11:52
[07/09 19:06:13] d2.evaluation.evaluator INFO: Inference done 35/500. Dataloading: 0.0020 s/iter. Inference: 1.5148 s/iter. Eval: 0.0007 s/iter. Total: 1.5175 s/iter. ETA=0:11:45
[07/09 19:06:19] d2.evaluation.evaluator INFO: Inference done 39/500. Dataloading: 0.0020 s/iter. Inference: 1.5165 s/iter. Eval: 0.0006 s/iter. Total: 1.5192 s/iter. ETA=0:11:40
[07/09 19:06:25] d2.evaluation.evaluator INFO: Inference done 43/500. Dataloading: 0.0020 s/iter. Inference: 1.5174 s/iter. Eval: 0.0006 s/iter. Total: 1.5201 s/iter. ETA=0:11:34
[07/09 19:06:31] d2.evaluation.evaluator INFO: Inference done 47/500. Dataloading: 0.0021 s/iter. Inference: 1.5174 s/iter. Eval: 0.0006 s/iter. Total: 1.5201 s/iter. ETA=0:11:28
[07/09 19:06:38] d2.evaluation.evaluator INFO: Inference done 51/500. Dataloading: 0.0021 s/iter. Inference: 1.5176 s/iter. Eval: 0.0006 s/iter. Total: 1.5204 s/iter. ETA=0:11:22
[07/09 19:06:44] d2.evaluation.evaluator INFO: Inference done 55/500. Dataloading: 0.0021 s/iter. Inference: 1.5181 s/iter. Eval: 0.0006 s/iter. Total: 1.5209 s/iter. ETA=0:11:16
[07/09 19:06:50] d2.evaluation.evaluator INFO: Inference done 59/500. Dataloading: 0.0021 s/iter. Inference: 1.5179 s/iter. Eval: 0.0006 s/iter. Total: 1.5207 s/iter. ETA=0:11:10
[07/09 19:06:56] d2.evaluation.evaluator INFO: Inference done 63/500. Dataloading: 0.0021 s/iter. Inference: 1.5173 s/iter. Eval: 0.0006 s/iter. Total: 1.5201 s/iter. ETA=0:11:04
[07/09 19:07:02] d2.evaluation.evaluator INFO: Inference done 67/500. Dataloading: 0.0021 s/iter. Inference: 1.5166 s/iter. Eval: 0.0006 s/iter. Total: 1.5194 s/iter. ETA=0:10:57
[07/09 19:07:08] d2.evaluation.evaluator INFO: Inference done 71/500. Dataloading: 0.0022 s/iter. Inference: 1.5162 s/iter. Eval: 0.0006 s/iter. Total: 1.5190 s/iter. ETA=0:10:51
[07/09 19:07:14] d2.evaluation.evaluator INFO: Inference done 75/500. Dataloading: 0.0022 s/iter. Inference: 1.5156 s/iter. Eval: 0.0006 s/iter. Total: 1.5184 s/iter. ETA=0:10:45
[07/09 19:07:20] d2.evaluation.evaluator INFO: Inference done 79/500. Dataloading: 0.0022 s/iter. Inference: 1.5172 s/iter. Eval: 0.0006 s/iter. Total: 1.5201 s/iter. ETA=0:10:39
[07/09 19:07:26] d2.evaluation.evaluator INFO: Inference done 83/500. Dataloading: 0.0022 s/iter. Inference: 1.5178 s/iter. Eval: 0.0007 s/iter. Total: 1.5207 s/iter. ETA=0:10:34
[07/09 19:07:32] d2.evaluation.evaluator INFO: Inference done 87/500. Dataloading: 0.0022 s/iter. Inference: 1.5194 s/iter. Eval: 0.0007 s/iter. Total: 1.5223 s/iter. ETA=0:10:28
[07/09 19:07:39] d2.evaluation.evaluator INFO: Inference done 91/500. Dataloading: 0.0023 s/iter. Inference: 1.5191 s/iter. Eval: 0.0007 s/iter. Total: 1.5221 s/iter. ETA=0:10:22
[07/09 19:07:44] d2.evaluation.evaluator INFO: Inference done 95/500. Dataloading: 0.0023 s/iter. Inference: 1.5174 s/iter. Eval: 0.0007 s/iter. Total: 1.5204 s/iter. ETA=0:10:15
[07/09 19:07:51] d2.evaluation.evaluator INFO: Inference done 99/500. Dataloading: 0.0023 s/iter. Inference: 1.5192 s/iter. Eval: 0.0007 s/iter. Total: 1.5223 s/iter. ETA=0:10:10
[07/09 19:07:57] d2.evaluation.evaluator INFO: Inference done 103/500. Dataloading: 0.0023 s/iter. Inference: 1.5182 s/iter. Eval: 0.0007 s/iter. Total: 1.5212 s/iter. ETA=0:10:03
[07/09 19:08:03] d2.evaluation.evaluator INFO: Inference done 107/500. Dataloading: 0.0023 s/iter. Inference: 1.5178 s/iter. Eval: 0.0007 s/iter. Total: 1.5208 s/iter. ETA=0:09:57
[07/09 19:08:09] d2.evaluation.evaluator INFO: Inference done 111/500. Dataloading: 0.0023 s/iter. Inference: 1.5181 s/iter. Eval: 0.0007 s/iter. Total: 1.5211 s/iter. ETA=0:09:51
[07/09 19:08:15] d2.evaluation.evaluator INFO: Inference done 115/500. Dataloading: 0.0023 s/iter. Inference: 1.5163 s/iter. Eval: 0.0007 s/iter. Total: 1.5194 s/iter. ETA=0:09:44
[07/09 19:08:21] d2.evaluation.evaluator INFO: Inference done 119/500. Dataloading: 0.0023 s/iter. Inference: 1.5147 s/iter. Eval: 0.0007 s/iter. Total: 1.5177 s/iter. ETA=0:09:38
[07/09 19:08:27] d2.evaluation.evaluator INFO: Inference done 123/500. Dataloading: 0.0023 s/iter. Inference: 1.5151 s/iter. Eval: 0.0007 s/iter. Total: 1.5181 s/iter. ETA=0:09:32
[07/09 19:08:33] d2.evaluation.evaluator INFO: Inference done 127/500. Dataloading: 0.0023 s/iter. Inference: 1.5150 s/iter. Eval: 0.0007 s/iter. Total: 1.5180 s/iter. ETA=0:09:26
[07/09 19:08:39] d2.evaluation.evaluator INFO: Inference done 131/500. Dataloading: 0.0023 s/iter. Inference: 1.5161 s/iter. Eval: 0.0007 s/iter. Total: 1.5191 s/iter. ETA=0:09:20
[07/09 19:08:45] d2.evaluation.evaluator INFO: Inference done 135/500. Dataloading: 0.0023 s/iter. Inference: 1.5170 s/iter. Eval: 0.0007 s/iter. Total: 1.5200 s/iter. ETA=0:09:14
[07/09 19:08:51] d2.evaluation.evaluator INFO: Inference done 139/500. Dataloading: 0.0023 s/iter. Inference: 1.5178 s/iter. Eval: 0.0007 s/iter. Total: 1.5209 s/iter. ETA=0:09:09
[07/09 19:08:58] d2.evaluation.evaluator INFO: Inference done 143/500. Dataloading: 0.0023 s/iter. Inference: 1.5185 s/iter. Eval: 0.0007 s/iter. Total: 1.5215 s/iter. ETA=0:09:03
[07/09 19:09:04] d2.evaluation.evaluator INFO: Inference done 147/500. Dataloading: 0.0023 s/iter. Inference: 1.5189 s/iter. Eval: 0.0007 s/iter. Total: 1.5219 s/iter. ETA=0:08:57
[07/09 19:09:10] d2.evaluation.evaluator INFO: Inference done 151/500. Dataloading: 0.0023 s/iter. Inference: 1.5193 s/iter. Eval: 0.0006 s/iter. Total: 1.5223 s/iter. ETA=0:08:51
[07/09 19:09:16] d2.evaluation.evaluator INFO: Inference done 155/500. Dataloading: 0.0023 s/iter. Inference: 1.5199 s/iter. Eval: 0.0006 s/iter. Total: 1.5229 s/iter. ETA=0:08:45
[07/09 19:09:22] d2.evaluation.evaluator INFO: Inference done 159/500. Dataloading: 0.0023 s/iter. Inference: 1.5207 s/iter. Eval: 0.0006 s/iter. Total: 1.5237 s/iter. ETA=0:08:39
[07/09 19:09:28] d2.evaluation.evaluator INFO: Inference done 163/500. Dataloading: 0.0023 s/iter. Inference: 1.5213 s/iter. Eval: 0.0006 s/iter. Total: 1.5243 s/iter. ETA=0:08:33
[07/09 19:09:35] d2.evaluation.evaluator INFO: Inference done 167/500. Dataloading: 0.0023 s/iter. Inference: 1.5217 s/iter. Eval: 0.0006 s/iter. Total: 1.5247 s/iter. ETA=0:08:27
[07/09 19:09:41] d2.evaluation.evaluator INFO: Inference done 171/500. Dataloading: 0.0024 s/iter. Inference: 1.5222 s/iter. Eval: 0.0006 s/iter. Total: 1.5254 s/iter. ETA=0:08:21
[07/09 19:09:47] d2.evaluation.evaluator INFO: Inference done 175/500. Dataloading: 0.0024 s/iter. Inference: 1.5207 s/iter. Eval: 0.0006 s/iter. Total: 1.5239 s/iter. ETA=0:08:15
[07/09 19:09:53] d2.evaluation.evaluator INFO: Inference done 179/500. Dataloading: 0.0024 s/iter. Inference: 1.5198 s/iter. Eval: 0.0006 s/iter. Total: 1.5229 s/iter. ETA=0:08:08
[07/09 19:09:59] d2.evaluation.evaluator INFO: Inference done 183/500. Dataloading: 0.0024 s/iter. Inference: 1.5186 s/iter. Eval: 0.0006 s/iter. Total: 1.5217 s/iter. ETA=0:08:02
[07/09 19:10:04] d2.evaluation.evaluator INFO: Inference done 187/500. Dataloading: 0.0024 s/iter. Inference: 1.5172 s/iter. Eval: 0.0006 s/iter. Total: 1.5203 s/iter. ETA=0:07:55
[07/09 19:10:10] d2.evaluation.evaluator INFO: Inference done 191/500. Dataloading: 0.0024 s/iter. Inference: 1.5159 s/iter. Eval: 0.0006 s/iter. Total: 1.5190 s/iter. ETA=0:07:49
[07/09 19:10:16] d2.evaluation.evaluator INFO: Inference done 195/500. Dataloading: 0.0024 s/iter. Inference: 1.5146 s/iter. Eval: 0.0006 s/iter. Total: 1.5177 s/iter. ETA=0:07:42
[07/09 19:10:22] d2.evaluation.evaluator INFO: Inference done 199/500. Dataloading: 0.0024 s/iter. Inference: 1.5135 s/iter. Eval: 0.0006 s/iter. Total: 1.5165 s/iter. ETA=0:07:36
[07/09 19:10:28] d2.evaluation.evaluator INFO: Inference done 203/500. Dataloading: 0.0024 s/iter. Inference: 1.5123 s/iter. Eval: 0.0006 s/iter. Total: 1.5154 s/iter. ETA=0:07:30
[07/09 19:10:34] d2.evaluation.evaluator INFO: Inference done 207/500. Dataloading: 0.0024 s/iter. Inference: 1.5115 s/iter. Eval: 0.0006 s/iter. Total: 1.5146 s/iter. ETA=0:07:23
[07/09 19:10:40] d2.evaluation.evaluator INFO: Inference done 211/500. Dataloading: 0.0024 s/iter. Inference: 1.5110 s/iter. Eval: 0.0006 s/iter. Total: 1.5140 s/iter. ETA=0:07:17
[07/09 19:10:45] d2.evaluation.evaluator INFO: Inference done 215/500. Dataloading: 0.0024 s/iter. Inference: 1.5104 s/iter. Eval: 0.0006 s/iter. Total: 1.5135 s/iter. ETA=0:07:11
[07/09 19:10:51] d2.evaluation.evaluator INFO: Inference done 219/500. Dataloading: 0.0024 s/iter. Inference: 1.5100 s/iter. Eval: 0.0006 s/iter. Total: 1.5131 s/iter. ETA=0:07:05
[07/09 19:10:57] d2.evaluation.evaluator INFO: Inference done 223/500. Dataloading: 0.0024 s/iter. Inference: 1.5090 s/iter. Eval: 0.0006 s/iter. Total: 1.5121 s/iter. ETA=0:06:58
[07/09 19:11:03] d2.evaluation.evaluator INFO: Inference done 227/500. Dataloading: 0.0024 s/iter. Inference: 1.5081 s/iter. Eval: 0.0006 s/iter. Total: 1.5112 s/iter. ETA=0:06:52
[07/09 19:11:09] d2.evaluation.evaluator INFO: Inference done 231/500. Dataloading: 0.0024 s/iter. Inference: 1.5081 s/iter. Eval: 0.0006 s/iter. Total: 1.5111 s/iter. ETA=0:06:46
[07/09 19:11:15] d2.evaluation.evaluator INFO: Inference done 235/500. Dataloading: 0.0024 s/iter. Inference: 1.5078 s/iter. Eval: 0.0006 s/iter. Total: 1.5109 s/iter. ETA=0:06:40
[07/09 19:11:21] d2.evaluation.evaluator INFO: Inference done 239/500. Dataloading: 0.0024 s/iter. Inference: 1.5075 s/iter. Eval: 0.0006 s/iter. Total: 1.5106 s/iter. ETA=0:06:34
[07/09 19:11:27] d2.evaluation.evaluator INFO: Inference done 243/500. Dataloading: 0.0024 s/iter. Inference: 1.5072 s/iter. Eval: 0.0006 s/iter. Total: 1.5102 s/iter. ETA=0:06:28
[07/09 19:11:33] d2.evaluation.evaluator INFO: Inference done 247/500. Dataloading: 0.0024 s/iter. Inference: 1.5065 s/iter. Eval: 0.0006 s/iter. Total: 1.5095 s/iter. ETA=0:06:21
[07/09 19:11:39] d2.evaluation.evaluator INFO: Inference done 251/500. Dataloading: 0.0024 s/iter. Inference: 1.5061 s/iter. Eval: 0.0006 s/iter. Total: 1.5091 s/iter. ETA=0:06:15
[07/09 19:11:45] d2.evaluation.evaluator INFO: Inference done 255/500. Dataloading: 0.0024 s/iter. Inference: 1.5067 s/iter. Eval: 0.0006 s/iter. Total: 1.5097 s/iter. ETA=0:06:09
[07/09 19:11:51] d2.evaluation.evaluator INFO: Inference done 259/500. Dataloading: 0.0024 s/iter. Inference: 1.5071 s/iter. Eval: 0.0006 s/iter. Total: 1.5102 s/iter. ETA=0:06:03
[07/09 19:11:57] d2.evaluation.evaluator INFO: Inference done 263/500. Dataloading: 0.0024 s/iter. Inference: 1.5073 s/iter. Eval: 0.0006 s/iter. Total: 1.5103 s/iter. ETA=0:05:57
[07/09 19:12:03] d2.evaluation.evaluator INFO: Inference done 267/500. Dataloading: 0.0024 s/iter. Inference: 1.5072 s/iter. Eval: 0.0006 s/iter. Total: 1.5103 s/iter. ETA=0:05:51
[07/09 19:12:09] d2.evaluation.evaluator INFO: Inference done 271/500. Dataloading: 0.0024 s/iter. Inference: 1.5071 s/iter. Eval: 0.0006 s/iter. Total: 1.5101 s/iter. ETA=0:05:45
[07/09 19:12:15] d2.evaluation.evaluator INFO: Inference done 275/500. Dataloading: 0.0024 s/iter. Inference: 1.5068 s/iter. Eval: 0.0006 s/iter. Total: 1.5098 s/iter. ETA=0:05:39
[07/09 19:12:21] d2.evaluation.evaluator INFO: Inference done 279/500. Dataloading: 0.0024 s/iter. Inference: 1.5065 s/iter. Eval: 0.0006 s/iter. Total: 1.5095 s/iter. ETA=0:05:33
[07/09 19:12:27] d2.evaluation.evaluator INFO: Inference done 283/500. Dataloading: 0.0024 s/iter. Inference: 1.5058 s/iter. Eval: 0.0006 s/iter. Total: 1.5089 s/iter. ETA=0:05:27
[07/09 19:12:33] d2.evaluation.evaluator INFO: Inference done 287/500. Dataloading: 0.0024 s/iter. Inference: 1.5052 s/iter. Eval: 0.0006 s/iter. Total: 1.5082 s/iter. ETA=0:05:21
[07/09 19:12:39] d2.evaluation.evaluator INFO: Inference done 291/500. Dataloading: 0.0024 s/iter. Inference: 1.5047 s/iter. Eval: 0.0006 s/iter. Total: 1.5077 s/iter. ETA=0:05:15
[07/09 19:12:45] d2.evaluation.evaluator INFO: Inference done 295/500. Dataloading: 0.0024 s/iter. Inference: 1.5050 s/iter. Eval: 0.0006 s/iter. Total: 1.5080 s/iter. ETA=0:05:09
[07/09 19:12:51] d2.evaluation.evaluator INFO: Inference done 299/500. Dataloading: 0.0024 s/iter. Inference: 1.5044 s/iter. Eval: 0.0006 s/iter. Total: 1.5074 s/iter. ETA=0:05:02
[07/09 19:12:57] d2.evaluation.evaluator INFO: Inference done 303/500. Dataloading: 0.0024 s/iter. Inference: 1.5038 s/iter. Eval: 0.0006 s/iter. Total: 1.5069 s/iter. ETA=0:04:56
[07/09 19:13:03] d2.evaluation.evaluator INFO: Inference done 307/500. Dataloading: 0.0024 s/iter. Inference: 1.5050 s/iter. Eval: 0.0006 s/iter. Total: 1.5081 s/iter. ETA=0:04:51
[07/09 19:13:10] d2.evaluation.evaluator INFO: Inference done 311/500. Dataloading: 0.0025 s/iter. Inference: 1.5067 s/iter. Eval: 0.0006 s/iter. Total: 1.5098 s/iter. ETA=0:04:45
[07/09 19:13:16] d2.evaluation.evaluator INFO: Inference done 315/500. Dataloading: 0.0025 s/iter. Inference: 1.5081 s/iter. Eval: 0.0006 s/iter. Total: 1.5112 s/iter. ETA=0:04:39
[07/09 19:13:23] d2.evaluation.evaluator INFO: Inference done 319/500. Dataloading: 0.0025 s/iter. Inference: 1.5095 s/iter. Eval: 0.0006 s/iter. Total: 1.5126 s/iter. ETA=0:04:33
[07/09 19:13:29] d2.evaluation.evaluator INFO: Inference done 323/500. Dataloading: 0.0025 s/iter. Inference: 1.5109 s/iter. Eval: 0.0006 s/iter. Total: 1.5140 s/iter. ETA=0:04:27
[07/09 19:13:36] d2.evaluation.evaluator INFO: Inference done 327/500. Dataloading: 0.0025 s/iter. Inference: 1.5122 s/iter. Eval: 0.0006 s/iter. Total: 1.5153 s/iter. ETA=0:04:22
[07/09 19:13:42] d2.evaluation.evaluator INFO: Inference done 331/500. Dataloading: 0.0025 s/iter. Inference: 1.5134 s/iter. Eval: 0.0006 s/iter. Total: 1.5165 s/iter. ETA=0:04:16
[07/09 19:13:49] d2.evaluation.evaluator INFO: Inference done 335/500. Dataloading: 0.0025 s/iter. Inference: 1.5147 s/iter. Eval: 0.0006 s/iter. Total: 1.5178 s/iter. ETA=0:04:10
[07/09 19:13:55] d2.evaluation.evaluator INFO: Inference done 339/500. Dataloading: 0.0025 s/iter. Inference: 1.5159 s/iter. Eval: 0.0006 s/iter. Total: 1.5190 s/iter. ETA=0:04:04
[07/09 19:14:01] d2.evaluation.evaluator INFO: Inference done 343/500. Dataloading: 0.0025 s/iter. Inference: 1.5169 s/iter. Eval: 0.0006 s/iter. Total: 1.5201 s/iter. ETA=0:03:58
[07/09 19:14:08] d2.evaluation.evaluator INFO: Inference done 347/500. Dataloading: 0.0025 s/iter. Inference: 1.5180 s/iter. Eval: 0.0006 s/iter. Total: 1.5212 s/iter. ETA=0:03:52
[07/09 19:14:14] d2.evaluation.evaluator INFO: Inference done 351/500. Dataloading: 0.0025 s/iter. Inference: 1.5192 s/iter. Eval: 0.0006 s/iter. Total: 1.5224 s/iter. ETA=0:03:46
[07/09 19:14:21] d2.evaluation.evaluator INFO: Inference done 355/500. Dataloading: 0.0025 s/iter. Inference: 1.5202 s/iter. Eval: 0.0006 s/iter. Total: 1.5233 s/iter. ETA=0:03:40
[07/09 19:14:27] d2.evaluation.evaluator INFO: Inference done 359/500. Dataloading: 0.0025 s/iter. Inference: 1.5213 s/iter. Eval: 0.0006 s/iter. Total: 1.5244 s/iter. ETA=0:03:34
[07/09 19:14:34] d2.evaluation.evaluator INFO: Inference done 363/500. Dataloading: 0.0025 s/iter. Inference: 1.5225 s/iter. Eval: 0.0006 s/iter. Total: 1.5256 s/iter. ETA=0:03:29
[07/09 19:14:40] d2.evaluation.evaluator INFO: Inference done 367/500. Dataloading: 0.0025 s/iter. Inference: 1.5236 s/iter. Eval: 0.0006 s/iter. Total: 1.5267 s/iter. ETA=0:03:23
[07/09 19:14:46] d2.evaluation.evaluator INFO: Inference done 371/500. Dataloading: 0.0025 s/iter. Inference: 1.5237 s/iter. Eval: 0.0006 s/iter. Total: 1.5268 s/iter. ETA=0:03:16
[07/09 19:14:52] d2.evaluation.evaluator INFO: Inference done 375/500. Dataloading: 0.0025 s/iter. Inference: 1.5230 s/iter. Eval: 0.0006 s/iter. Total: 1.5261 s/iter. ETA=0:03:10
[07/09 19:14:58] d2.evaluation.evaluator INFO: Inference done 379/500. Dataloading: 0.0025 s/iter. Inference: 1.5223 s/iter. Eval: 0.0006 s/iter. Total: 1.5254 s/iter. ETA=0:03:04
[07/09 19:15:04] d2.evaluation.evaluator INFO: Inference done 383/500. Dataloading: 0.0025 s/iter. Inference: 1.5217 s/iter. Eval: 0.0006 s/iter. Total: 1.5248 s/iter. ETA=0:02:58
[07/09 19:15:10] d2.evaluation.evaluator INFO: Inference done 387/500. Dataloading: 0.0025 s/iter. Inference: 1.5211 s/iter. Eval: 0.0006 s/iter. Total: 1.5242 s/iter. ETA=0:02:52
[07/09 19:15:16] d2.evaluation.evaluator INFO: Inference done 391/500. Dataloading: 0.0025 s/iter. Inference: 1.5204 s/iter. Eval: 0.0006 s/iter. Total: 1.5235 s/iter. ETA=0:02:46
[07/09 19:15:22] d2.evaluation.evaluator INFO: Inference done 395/500. Dataloading: 0.0025 s/iter. Inference: 1.5198 s/iter. Eval: 0.0006 s/iter. Total: 1.5229 s/iter. ETA=0:02:39
[07/09 19:15:27] d2.evaluation.evaluator INFO: Inference done 399/500. Dataloading: 0.0025 s/iter. Inference: 1.5191 s/iter. Eval: 0.0006 s/iter. Total: 1.5222 s/iter. ETA=0:02:33
[07/09 19:15:33] d2.evaluation.evaluator INFO: Inference done 403/500. Dataloading: 0.0025 s/iter. Inference: 1.5185 s/iter. Eval: 0.0006 s/iter. Total: 1.5216 s/iter. ETA=0:02:27
[07/09 19:15:39] d2.evaluation.evaluator INFO: Inference done 407/500. Dataloading: 0.0025 s/iter. Inference: 1.5179 s/iter. Eval: 0.0006 s/iter. Total: 1.5210 s/iter. ETA=0:02:21
[07/09 19:15:45] d2.evaluation.evaluator INFO: Inference done 411/500. Dataloading: 0.0025 s/iter. Inference: 1.5173 s/iter. Eval: 0.0006 s/iter. Total: 1.5204 s/iter. ETA=0:02:15
[07/09 19:15:51] d2.evaluation.evaluator INFO: Inference done 415/500. Dataloading: 0.0025 s/iter. Inference: 1.5167 s/iter. Eval: 0.0006 s/iter. Total: 1.5202 s/iter. ETA=0:02:09
[07/09 19:15:57] d2.evaluation.evaluator INFO: Inference done 419/500. Dataloading: 0.0025 s/iter. Inference: 1.5162 s/iter. Eval: 0.0006 s/iter. Total: 1.5196 s/iter. ETA=0:02:03
[07/09 19:16:03] d2.evaluation.evaluator INFO: Inference done 423/500. Dataloading: 0.0025 s/iter. Inference: 1.5156 s/iter. Eval: 0.0006 s/iter. Total: 1.5191 s/iter. ETA=0:01:56
[07/09 19:16:08] d2.evaluation.evaluator INFO: Inference done 427/500. Dataloading: 0.0025 s/iter. Inference: 1.5152 s/iter. Eval: 0.0006 s/iter. Total: 1.5186 s/iter. ETA=0:01:50
[07/09 19:16:14] d2.evaluation.evaluator INFO: Inference done 431/500. Dataloading: 0.0025 s/iter. Inference: 1.5146 s/iter. Eval: 0.0006 s/iter. Total: 1.5181 s/iter. ETA=0:01:44
[07/09 19:16:20] d2.evaluation.evaluator INFO: Inference done 435/500. Dataloading: 0.0025 s/iter. Inference: 1.5141 s/iter. Eval: 0.0006 s/iter. Total: 1.5175 s/iter. ETA=0:01:38
[07/09 19:16:26] d2.evaluation.evaluator INFO: Inference done 439/500. Dataloading: 0.0025 s/iter. Inference: 1.5136 s/iter. Eval: 0.0006 s/iter. Total: 1.5170 s/iter. ETA=0:01:32
[07/09 19:16:32] d2.evaluation.evaluator INFO: Inference done 443/500. Dataloading: 0.0025 s/iter. Inference: 1.5131 s/iter. Eval: 0.0006 s/iter. Total: 1.5165 s/iter. ETA=0:01:26
[07/09 19:16:38] d2.evaluation.evaluator INFO: Inference done 447/500. Dataloading: 0.0025 s/iter. Inference: 1.5126 s/iter. Eval: 0.0006 s/iter. Total: 1.5160 s/iter. ETA=0:01:20
[07/09 19:16:44] d2.evaluation.evaluator INFO: Inference done 451/500. Dataloading: 0.0024 s/iter. Inference: 1.5122 s/iter. Eval: 0.0006 s/iter. Total: 1.5156 s/iter. ETA=0:01:14
[07/09 19:16:50] d2.evaluation.evaluator INFO: Inference done 455/500. Dataloading: 0.0024 s/iter. Inference: 1.5122 s/iter. Eval: 0.0006 s/iter. Total: 1.5156 s/iter. ETA=0:01:08
[07/09 19:16:56] d2.evaluation.evaluator INFO: Inference done 459/500. Dataloading: 0.0025 s/iter. Inference: 1.5132 s/iter. Eval: 0.0006 s/iter. Total: 1.5167 s/iter. ETA=0:01:02
[07/09 19:17:03] d2.evaluation.evaluator INFO: Inference done 463/500. Dataloading: 0.0025 s/iter. Inference: 1.5141 s/iter. Eval: 0.0006 s/iter. Total: 1.5175 s/iter. ETA=0:00:56
[07/09 19:17:09] d2.evaluation.evaluator INFO: Inference done 467/500. Dataloading: 0.0025 s/iter. Inference: 1.5149 s/iter. Eval: 0.0006 s/iter. Total: 1.5184 s/iter. ETA=0:00:50
[07/09 19:17:16] d2.evaluation.evaluator INFO: Inference done 471/500. Dataloading: 0.0025 s/iter. Inference: 1.5157 s/iter. Eval: 0.0006 s/iter. Total: 1.5191 s/iter. ETA=0:00:44
[07/09 19:17:22] d2.evaluation.evaluator INFO: Inference done 475/500. Dataloading: 0.0025 s/iter. Inference: 1.5165 s/iter. Eval: 0.0006 s/iter. Total: 1.5200 s/iter. ETA=0:00:37
[07/09 19:17:28] d2.evaluation.evaluator INFO: Inference done 479/500. Dataloading: 0.0025 s/iter. Inference: 1.5174 s/iter. Eval: 0.0006 s/iter. Total: 1.5208 s/iter. ETA=0:00:31
[07/09 19:17:35] d2.evaluation.evaluator INFO: Inference done 483/500. Dataloading: 0.0025 s/iter. Inference: 1.5182 s/iter. Eval: 0.0006 s/iter. Total: 1.5216 s/iter. ETA=0:00:25
[07/09 19:17:41] d2.evaluation.evaluator INFO: Inference done 487/500. Dataloading: 0.0025 s/iter. Inference: 1.5190 s/iter. Eval: 0.0006 s/iter. Total: 1.5224 s/iter. ETA=0:00:19
[07/09 19:17:48] d2.evaluation.evaluator INFO: Inference done 491/500. Dataloading: 0.0025 s/iter. Inference: 1.5198 s/iter. Eval: 0.0006 s/iter. Total: 1.5232 s/iter. ETA=0:00:13
[07/09 19:17:54] d2.evaluation.evaluator INFO: Inference done 495/500. Dataloading: 0.0025 s/iter. Inference: 1.5205 s/iter. Eval: 0.0006 s/iter. Total: 1.5239 s/iter. ETA=0:00:07
[07/09 19:18:01] d2.evaluation.evaluator INFO: Inference done 499/500. Dataloading: 0.0025 s/iter. Inference: 1.5212 s/iter. Eval: 0.0006 s/iter. Total: 1.5246 s/iter. ETA=0:00:01
[07/09 19:18:03] d2.evaluation.evaluator INFO: Total inference time: 0:12:34.942448 (1.525136 s / iter per device, on 1 devices)
[07/09 19:18:03] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:12:33 (1.521410 s / iter per device, on 1 devices)
[07/09 19:18:03] adet.evaluation.text_evaluation_all INFO: Saving results to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[07/09 19:18:05] d2.engine.defaults INFO: Evaluation results for ic15_test in csv format:
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: Task: DETECTION_ONLY_RESULTS
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: 0.9232,0.8792,0.9006
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: Task: None-E2E_RESULTS
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: 0.7588,0.7227,0.7403
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: Task: Strong-E2E_RESULTS
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 19:18:05] d2.evaluation.testing INFO: copypaste: 0.9504,0.8209,0.8809
[07/09 19:49:17] detectron2 INFO: Rank of current process: 0. World size: 1
[07/09 19:49:17] detectron2 INFO: Environment info:
----------------------  --------------------------------------------------------------------------------------
sys.platform            linux
Python                  3.9.19 (main, May  6 2024, 19:43:03) [GCC 11.2.0]
numpy                   1.26.4
detectron2              0.6 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/detectron2
Compiler                GCC 7.3
CUDA compiler           CUDA 11.3
detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6
DETECTRON2_ENV_MODULE   <not set>
PyTorch                 1.10.1 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torch
PyTorch debug build     False
GPU available           Yes
GPU 0                   NVIDIA GeForce RTX 3060 (arch=8.6)
Driver version          470.256.02
CUDA_HOME               /usr/local/cuda
Pillow                  9.0.1
torchvision             0.11.2 @/home/kylee/anaconda3/envs/py39_abinet/lib/python3.9/site-packages/torchvision
torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                  0.1.5.post20221221
iopath                  0.1.9
cv2                     4.10.0
----------------------  --------------------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.2
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

[07/09 19:49:17] detectron2 INFO: Command line arguments: Namespace(config_file='configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml', resume=False, eval_only=True, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50152', opts=['MODEL.WEIGHTS', 'weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth'])
[07/09 19:49:17] detectron2 INFO: Contents of args.config_file=configs/ViTAEv2_S/IC15/finetune_150k_tt_mlt_13_15_textocr.yaml:
_BASE_: "../Base_det.yaml"

MODEL:
  WEIGHTS: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/pretrain/model_final.pth"
  ViTAEv2:
    DROP_PATH_RATE: 0.2
  TRANSFORMER:
    INFERENCE_TH_TEST: 0.3

DATASETS:
  TRAIN: ("ic15_train",)
  TEST: ("ic15_test",)

INPUT:
  MIN_SIZE_TRAIN: (800,900,1000,1100,1200,1300,1400)
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MAX_SIZE_TEST: 4000
  CROP:
    ENABLED: False
  ROTATE: False

SOLVER:
  IMS_PER_BATCH: 8
  BASE_LR: 1e-5
  LR_BACKBONE: 1e-5
  WARMUP_ITERS: 0
  STEPS: (100000,)
  MAX_ITER: 1000
  CHECKPOINT_PERIOD: 1000

TEST:
  EVAL_PERIOD: 500
  # 1 - Generic, 2 - Weak, 3 - Strong (for icdar2015)
  LEXICON_TYPE: 3

OUTPUT_DIR: "output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15"
[07/09 19:49:17] detectron2 INFO: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 8
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - ic15_test
  TRAIN:
  - ic15_train
GLOBAL:
  HACK: 1.0
INPUT:
  CROP:
    CROP_INSTANCE: false
    ENABLED: false
    SIZE:
    - 0.1
    - 0.1
    TYPE: relative_range
  FORMAT: RGB
  HFLIP_TRAIN: false
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 4000
  MAX_SIZE_TRAIN: 3000
  MIN_SIZE_TEST: 1440
  MIN_SIZE_TRAIN:
  - 800
  - 900
  - 1000
  - 1100
  - 1200
  - 1300
  - 1400
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  ROTATE: false
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    ANTI_ALIAS: false
    FREEZE_AT: 2
    NAME: build_vitaev2_backbone
  BASIS_MODULE:
    ANN_SET: coco
    COMMON_STRIDE: 8
    CONVS_DIM: 128
    IN_FEATURES:
    - p3
    - p4
    - p5
    LOSS_ON: false
    LOSS_WEIGHT: 0.3
    NAME: ProtoNet
    NORM: SyncBN
    NUM_BASES: 4
    NUM_CLASSES: 80
    NUM_CONVS: 3
  BATEXT:
    CANONICAL_SIZE: 96
    CONV_DIM: 256
    CUSTOM_DICT: ''
    IN_FEATURES:
    - p2
    - p3
    - p4
    NUM_CHARS: 25
    NUM_CONV: 2
    POOLER_RESOLUTION:
    - 8
    - 32
    POOLER_SCALES:
    - 0.25
    - 0.125
    - 0.0625
    RECOGNITION_LOSS: ctc
    RECOGNIZER: attn
    SAMPLING_RATIO: 1
    USE_AET: false
    USE_COORDCONV: false
    VOC_SIZE: 96
  DEVICE: cuda
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: TransformerPureDetector
  MOBILENET: false
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 123.675
  - 116.28
  - 103.53
  PIXEL_STD:
  - 58.395
  - 57.12
  - 57.375
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_INTERVAL: 1
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES5_DILATION: 1
    STEM_OUT_CHANNELS: 64
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SEM_SEG_HEAD:
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
  SWIN:
    DROP_PATH_RATE: 0.2
    TYPE: tiny
  TOP_MODULE:
    DIM: 16
    NAME: conv
  TRANSFORMER:
    AUX_LOSS: true
    BOUNDARY_HEAD: true
    CUSTOM_DICT: ''
    DEC_LAYERS: 6
    DEC_N_POINTS: 4
    DIM_FEEDFORWARD: 1024
    DROPOUT: 0.0
    ENABLED: true
    ENC_LAYERS: 6
    ENC_N_POINTS: 4
    HIDDEN_DIM: 256
    INFERENCE_TH_TEST: 0.3
    LOSS:
      AUX_LOSS: true
      BEZIER_CLASS_WEIGHT: 1.0
      BEZIER_COORD_WEIGHT: 1.0
      BEZIER_SAMPLE_POINTS: 25
      BOUNDARY_WEIGHT: 0.5
      FOCAL_ALPHA: 0.25
      FOCAL_GAMMA: 2.0
      POINT_CLASS_WEIGHT: 1.0
      POINT_COORD_WEIGHT: 1.0
      POINT_TEXT_WEIGHT: 0.5
    NHEADS: 8
    NUM_FEATURE_LEVELS: 4
    NUM_POINTS: 25
    NUM_QUERIES: 100
    POSITION_EMBEDDING_SCALE: 6.283185307179586
    TEMPERATURE: 10000
    VOC_SIZE: 37
  ViTAEv2:
    DROP_PATH_RATE: 0.2
    TYPE: vitaev2_s
  WEIGHTS: weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth
OUTPUT_DIR: output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15
SEED: 42
SOLVER:
  AMP:
    ENABLED: false
  BASE_LR: 1.0e-05
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 1000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.1
    ENABLED: true
    NORM_TYPE: 2.0
  GAMMA: 0.1
  IMS_PER_BATCH: 8
  LR_BACKBONE: 1.0e-05
  LR_BACKBONE_NAMES:
  - backbone.0
  LR_LINEAR_PROJ_MULT: 1.0
  LR_LINEAR_PROJ_NAMES:
  - reference_points
  - sampling_offsets
  LR_SCHEDULER_NAME: WarmupMultiStepLR
  MAX_ITER: 1000
  MOMENTUM: 0.9
  NESTEROV: false
  OPTIMIZER: ADAMW
  REFERENCE_WORLD_SIZE: 0
  STEPS:
  - 100000
  WARMUP_FACTOR: 0.001
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4000
    MIN_SIZES:
    - 400
    - 500
    - 600
    - 700
    - 800
    - 900
    - 1000
    - 1100
    - 1200
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 500
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  LEXICON_TYPE: 3
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0

[07/09 19:49:17] detectron2 INFO: Full config saved to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/config.yaml
[07/09 19:49:23] d2.engine.defaults INFO: Model:
TransformerPureDetector(
  (detection_transformer): DETECTION_TRANSFORMER(
    (backbone): Joiner(
      (0): MaskedBackbone(
        (backbone): ViTAEv2(
          (layers): ModuleList(
            (0): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(5, 5), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(8, 8), dilation=(3, 3))
                      (1): GELU()
                    )
                    (3): Sequential(
                      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(11, 11), dilation=(4, 4))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=256, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=64, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=64, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=64, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=64, out_features=192, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=64, out_features=64, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.015)
                  (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=64, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=64, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                  )
                )
              )
            )
            (1): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16)
                  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                    (2): Sequential(
                      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(3, 3), dilation=(3, 3))
                      (1): GELU()
                    )
                  )
                )
                (attn): WindowTransformerBlock(
                  (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=1
                    (qkv): Linear(in_features=192, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=128, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=128, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (attn): WindowAttention(
                    dim=128, window_size=(7, 7), num_heads=2
                    (qkv): Linear(in_features=128, out_features=384, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=128, out_features=128, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                    (softmax): Softmax(dim=-1)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=128, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=128, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  )
                )
              )
            )
            (2): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32)
                  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                  (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=256, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=256, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.031)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.046)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (2): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.062)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (3): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.077)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (4): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.092)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (5): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.108)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (6): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
                (7): NormalCell(
                  (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=256, out_features=768, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=256, out_features=256, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=256, out_features=1024, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=1024, out_features=256, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  )
                )
              )
            )
            (3): BasicLayer(
              (RC): ReductionCell(
                (PCM): Sequential(
                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
                  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (2): SiLU(inplace=True)
                  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                  (5): SiLU(inplace=True)
                  (6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
                )
                (PRM): PRM(
                  (convs): ModuleList(
                    (0): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (1): GELU()
                    )
                    (1): Sequential(
                      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), dilation=(2, 2))
                      (1): GELU()
                    )
                  )
                )
                (attn): Token_transformer(
                  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=False)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): Identity()
                  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=512, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=512, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (NC): ModuleList(
                (0): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.123)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
                (1): NormalCell(
                  (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (attn): Attention(
                    (qkv): Linear(in_features=512, out_features=1536, bias=True)
                    (attn_drop): Dropout(p=0.0, inplace=False)
                    (proj): Linear(in_features=512, out_features=512, bias=True)
                    (proj_drop): Dropout(p=0.0, inplace=False)
                  )
                  (drop_path): DropPath(drop_prob=0.138)
                  (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
                  (mlp): Mlp(
                    (fc1): Linear(in_features=512, out_features=2048, bias=True)
                    (act): GELU()
                    (fc2): Linear(in_features=2048, out_features=512, bias=True)
                    (drop): Dropout(p=0.0, inplace=False)
                  )
                  (PCM): Sequential(
                    (0): Conv2d(512, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (2): SiLU(inplace=True)
                    (3): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                    (5): SiLU(inplace=True)
                    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                  )
                )
              )
            )
          )
        )
      )
      (1): PositionalEncoding2D()
    )
    (point_embed): Embedding(2500, 256)
    (transformer): DeformableTransformer(
      (encoder): DeformableTransformerEncoder(
        (layers): ModuleList(
          (0): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableTransformerEncoderLayer(
            (self_attn): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout2): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
      (decoder): DeformableCompositeTransformerDecoder(
        (layers): ModuleList(
          (0): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (1): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (2): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (3): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (4): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
          (5): DeformableCompositeTransformerDecoderLayer(
            (attn_intra): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (norm_intra): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (dropout_intra): Dropout(p=0.0, inplace=False)
            (attn_inter): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
            )
            (dropout_inter): Dropout(p=0.0, inplace=False)
            (norm_inter): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn_cross): MSDeformAttn(
              (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
              (attention_weights): Linear(in_features=256, out_features=128, bias=True)
              (value_proj): Linear(in_features=256, out_features=256, bias=True)
              (output_proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (dropout_cross): Dropout(p=0.0, inplace=False)
            (norm_cross): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (linear1): Linear(in_features=256, out_features=1024, bias=True)
            (dropout3): Dropout(p=0.0, inplace=False)
            (linear2): Linear(in_features=1024, out_features=256, bias=True)
            (dropout4): Dropout(p=0.0, inplace=False)
            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          )
        )
        (ref_point_head): MLP(
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=256, bias=True)
            (1): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (ctrl_point_coord): ModuleList(
          (0): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (1): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (2): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (3): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (4): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
          (5): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=256, bias=True)
              (1): Linear(in_features=256, out_features=256, bias=True)
              (2): Linear(in_features=256, out_features=2, bias=True)
            )
          )
        )
      )
      (enc_output): Linear(in_features=256, out_features=256, bias=True)
      (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (bezier_coord_embed): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=8, bias=True)
        )
      )
      (bezier_class_embed): Linear(in_features=256, out_features=1, bias=True)
    )
    (input_proj): ModuleList(
      (0): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): GroupNorm(32, 256, eps=1e-05, affine=True)
      )
    )
    (bezier_proposal_coord): MLP(
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=8, bias=True)
      )
    )
    (bezier_proposal_class): Linear(in_features=256, out_features=1, bias=True)
    (ctrl_point_coord): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=2, bias=True)
        )
      )
    )
    (ctrl_point_class): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Linear(in_features=256, out_features=1, bias=True)
      (2): Linear(in_features=256, out_features=1, bias=True)
      (3): Linear(in_features=256, out_features=1, bias=True)
      (4): Linear(in_features=256, out_features=1, bias=True)
      (5): Linear(in_features=256, out_features=1, bias=True)
    )
    (ctrl_point_text): ModuleList(
      (0): Linear(in_features=256, out_features=38, bias=True)
      (1): Linear(in_features=256, out_features=38, bias=True)
      (2): Linear(in_features=256, out_features=38, bias=True)
      (3): Linear(in_features=256, out_features=38, bias=True)
      (4): Linear(in_features=256, out_features=38, bias=True)
      (5): Linear(in_features=256, out_features=38, bias=True)
    )
    (boundary_offset): ModuleList(
      (0): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (1): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (2): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (3): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (4): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
      (5): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=4, bias=True)
        )
      )
    )
  )
  (criterion): SetCriterion(
    (enc_matcher): BezierHungarianMatcher(
      (bezier_sampler): BezierSampler()
    )
    (dec_matcher): CtrlPointHungarianMatcher()
    (bezier_sampler): BezierSampler()
  )
)
[07/09 19:49:23] fvcore.common.checkpoint INFO: [Checkpointer] Loading from weights/vitaev2-s_pretrain_synth-tt-mlt-13-15-textocr.pth ...
[07/09 19:49:24] d2.data.dataset_mapper INFO: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(1440, 1440), max_size=4000, sample_style='choice')]
[07/09 19:49:24] adet.data.dataset_mapper INFO: Rebuilding the augmentations. The previous augmentations will be overridden.
[07/09 19:49:24] adet.data.datasets.text INFO: Loaded 500 images in COCO format from datasets/ic15/test.json
[07/09 19:49:24] d2.data.common INFO: Serializing 500 elements to byte tensors and concatenating them all ...
[07/09 19:49:24] d2.data.common INFO: Serialized dataset takes 0.06 MiB
[07/09 19:49:24] d2.evaluation.evaluator INFO: Start inference on 500 batches
[07/09 19:49:42] d2.evaluation.evaluator INFO: Inference done 11/500. Dataloading: 0.0018 s/iter. Inference: 1.4747 s/iter. Eval: 0.0008 s/iter. Total: 1.4772 s/iter. ETA=0:12:02
[07/09 19:49:48] d2.evaluation.evaluator INFO: Inference done 15/500. Dataloading: 0.0018 s/iter. Inference: 1.4952 s/iter. Eval: 0.0009 s/iter. Total: 1.4979 s/iter. ETA=0:12:06
[07/09 19:49:54] d2.evaluation.evaluator INFO: Inference done 19/500. Dataloading: 0.0018 s/iter. Inference: 1.4848 s/iter. Eval: 0.0009 s/iter. Total: 1.4875 s/iter. ETA=0:11:55
[07/09 19:49:59] d2.evaluation.evaluator INFO: Inference done 23/500. Dataloading: 0.0019 s/iter. Inference: 1.4792 s/iter. Eval: 0.0010 s/iter. Total: 1.4821 s/iter. ETA=0:11:46
[07/09 19:50:05] d2.evaluation.evaluator INFO: Inference done 27/500. Dataloading: 0.0019 s/iter. Inference: 1.4755 s/iter. Eval: 0.0009 s/iter. Total: 1.4784 s/iter. ETA=0:11:39
[07/09 19:50:11] d2.evaluation.evaluator INFO: Inference done 31/500. Dataloading: 0.0019 s/iter. Inference: 1.4757 s/iter. Eval: 0.0009 s/iter. Total: 1.4786 s/iter. ETA=0:11:33
[07/09 19:50:17] d2.evaluation.evaluator INFO: Inference done 35/500. Dataloading: 0.0020 s/iter. Inference: 1.4797 s/iter. Eval: 0.0009 s/iter. Total: 1.4826 s/iter. ETA=0:11:29
[07/09 19:50:23] d2.evaluation.evaluator INFO: Inference done 39/500. Dataloading: 0.0020 s/iter. Inference: 1.4852 s/iter. Eval: 0.0009 s/iter. Total: 1.4881 s/iter. ETA=0:11:26
[07/09 19:50:29] d2.evaluation.evaluator INFO: Inference done 43/500. Dataloading: 0.0020 s/iter. Inference: 1.4878 s/iter. Eval: 0.0008 s/iter. Total: 1.4907 s/iter. ETA=0:11:21
[07/09 19:50:36] d2.evaluation.evaluator INFO: Inference done 47/500. Dataloading: 0.0020 s/iter. Inference: 1.4918 s/iter. Eval: 0.0008 s/iter. Total: 1.4947 s/iter. ETA=0:11:17
[07/09 19:50:41] d2.evaluation.evaluator INFO: Inference done 51/500. Dataloading: 0.0020 s/iter. Inference: 1.4904 s/iter. Eval: 0.0008 s/iter. Total: 1.4934 s/iter. ETA=0:11:10
[07/09 19:50:47] d2.evaluation.evaluator INFO: Inference done 55/500. Dataloading: 0.0020 s/iter. Inference: 1.4883 s/iter. Eval: 0.0008 s/iter. Total: 1.4913 s/iter. ETA=0:11:03
[07/09 19:50:53] d2.evaluation.evaluator INFO: Inference done 59/500. Dataloading: 0.0021 s/iter. Inference: 1.4895 s/iter. Eval: 0.0008 s/iter. Total: 1.4924 s/iter. ETA=0:10:58
[07/09 19:50:59] d2.evaluation.evaluator INFO: Inference done 63/500. Dataloading: 0.0021 s/iter. Inference: 1.4886 s/iter. Eval: 0.0008 s/iter. Total: 1.4916 s/iter. ETA=0:10:51
[07/09 19:51:05] d2.evaluation.evaluator INFO: Inference done 67/500. Dataloading: 0.0021 s/iter. Inference: 1.4870 s/iter. Eval: 0.0008 s/iter. Total: 1.4899 s/iter. ETA=0:10:45
[07/09 19:51:11] d2.evaluation.evaluator INFO: Inference done 71/500. Dataloading: 0.0021 s/iter. Inference: 1.4852 s/iter. Eval: 0.0008 s/iter. Total: 1.4882 s/iter. ETA=0:10:38
[07/09 19:51:17] d2.evaluation.evaluator INFO: Inference done 75/500. Dataloading: 0.0021 s/iter. Inference: 1.4839 s/iter. Eval: 0.0008 s/iter. Total: 1.4869 s/iter. ETA=0:10:31
[07/09 19:51:23] d2.evaluation.evaluator INFO: Inference done 79/500. Dataloading: 0.0021 s/iter. Inference: 1.4831 s/iter. Eval: 0.0009 s/iter. Total: 1.4862 s/iter. ETA=0:10:25
[07/09 19:51:29] d2.evaluation.evaluator INFO: Inference done 83/500. Dataloading: 0.0021 s/iter. Inference: 1.4818 s/iter. Eval: 0.0009 s/iter. Total: 1.4848 s/iter. ETA=0:10:19
[07/09 19:51:34] d2.evaluation.evaluator INFO: Inference done 87/500. Dataloading: 0.0021 s/iter. Inference: 1.4807 s/iter. Eval: 0.0009 s/iter. Total: 1.4838 s/iter. ETA=0:10:12
[07/09 19:51:40] d2.evaluation.evaluator INFO: Inference done 91/500. Dataloading: 0.0021 s/iter. Inference: 1.4801 s/iter. Eval: 0.0009 s/iter. Total: 1.4832 s/iter. ETA=0:10:06
[07/09 19:51:46] d2.evaluation.evaluator INFO: Inference done 95/500. Dataloading: 0.0021 s/iter. Inference: 1.4804 s/iter. Eval: 0.0009 s/iter. Total: 1.4835 s/iter. ETA=0:10:00
[07/09 19:51:52] d2.evaluation.evaluator INFO: Inference done 99/500. Dataloading: 0.0021 s/iter. Inference: 1.4817 s/iter. Eval: 0.0009 s/iter. Total: 1.4847 s/iter. ETA=0:09:55
[07/09 19:51:59] d2.evaluation.evaluator INFO: Inference done 103/500. Dataloading: 0.0021 s/iter. Inference: 1.4857 s/iter. Eval: 0.0009 s/iter. Total: 1.4888 s/iter. ETA=0:09:51
[07/09 19:52:05] d2.evaluation.evaluator INFO: Inference done 107/500. Dataloading: 0.0021 s/iter. Inference: 1.4905 s/iter. Eval: 0.0009 s/iter. Total: 1.4935 s/iter. ETA=0:09:46
[07/09 19:52:12] d2.evaluation.evaluator INFO: Inference done 111/500. Dataloading: 0.0022 s/iter. Inference: 1.4950 s/iter. Eval: 0.0009 s/iter. Total: 1.4980 s/iter. ETA=0:09:42
[07/09 19:52:18] d2.evaluation.evaluator INFO: Inference done 115/500. Dataloading: 0.0022 s/iter. Inference: 1.4990 s/iter. Eval: 0.0009 s/iter. Total: 1.5021 s/iter. ETA=0:09:38
[07/09 19:52:24] d2.evaluation.evaluator INFO: Inference done 119/500. Dataloading: 0.0022 s/iter. Inference: 1.5024 s/iter. Eval: 0.0009 s/iter. Total: 1.5055 s/iter. ETA=0:09:33
[07/09 19:52:31] d2.evaluation.evaluator INFO: Inference done 123/500. Dataloading: 0.0022 s/iter. Inference: 1.5059 s/iter. Eval: 0.0009 s/iter. Total: 1.5090 s/iter. ETA=0:09:28
[07/09 19:52:37] d2.evaluation.evaluator INFO: Inference done 127/500. Dataloading: 0.0022 s/iter. Inference: 1.5092 s/iter. Eval: 0.0009 s/iter. Total: 1.5123 s/iter. ETA=0:09:24
[07/09 19:52:44] d2.evaluation.evaluator INFO: Inference done 131/500. Dataloading: 0.0022 s/iter. Inference: 1.5121 s/iter. Eval: 0.0009 s/iter. Total: 1.5152 s/iter. ETA=0:09:19
[07/09 19:52:50] d2.evaluation.evaluator INFO: Inference done 135/500. Dataloading: 0.0022 s/iter. Inference: 1.5148 s/iter. Eval: 0.0009 s/iter. Total: 1.5179 s/iter. ETA=0:09:14
[07/09 19:52:56] d2.evaluation.evaluator INFO: Inference done 139/500. Dataloading: 0.0022 s/iter. Inference: 1.5152 s/iter. Eval: 0.0009 s/iter. Total: 1.5184 s/iter. ETA=0:09:08
[07/09 19:53:02] d2.evaluation.evaluator INFO: Inference done 143/500. Dataloading: 0.0022 s/iter. Inference: 1.5148 s/iter. Eval: 0.0009 s/iter. Total: 1.5180 s/iter. ETA=0:09:01
[07/09 19:53:09] d2.evaluation.evaluator INFO: Inference done 147/500. Dataloading: 0.0022 s/iter. Inference: 1.5163 s/iter. Eval: 0.0009 s/iter. Total: 1.5195 s/iter. ETA=0:08:56
[07/09 19:53:15] d2.evaluation.evaluator INFO: Inference done 151/500. Dataloading: 0.0022 s/iter. Inference: 1.5183 s/iter. Eval: 0.0009 s/iter. Total: 1.5215 s/iter. ETA=0:08:51
[07/09 19:53:21] d2.evaluation.evaluator INFO: Inference done 155/500. Dataloading: 0.0022 s/iter. Inference: 1.5183 s/iter. Eval: 0.0009 s/iter. Total: 1.5215 s/iter. ETA=0:08:44
[07/09 19:53:27] d2.evaluation.evaluator INFO: Inference done 159/500. Dataloading: 0.0022 s/iter. Inference: 1.5176 s/iter. Eval: 0.0009 s/iter. Total: 1.5208 s/iter. ETA=0:08:38
[07/09 19:53:33] d2.evaluation.evaluator INFO: Inference done 163/500. Dataloading: 0.0022 s/iter. Inference: 1.5172 s/iter. Eval: 0.0009 s/iter. Total: 1.5203 s/iter. ETA=0:08:32
[07/09 19:53:39] d2.evaluation.evaluator INFO: Inference done 167/500. Dataloading: 0.0022 s/iter. Inference: 1.5178 s/iter. Eval: 0.0009 s/iter. Total: 1.5209 s/iter. ETA=0:08:26
[07/09 19:53:45] d2.evaluation.evaluator INFO: Inference done 171/500. Dataloading: 0.0022 s/iter. Inference: 1.5177 s/iter. Eval: 0.0009 s/iter. Total: 1.5208 s/iter. ETA=0:08:20
[07/09 19:53:51] d2.evaluation.evaluator INFO: Inference done 175/500. Dataloading: 0.0022 s/iter. Inference: 1.5174 s/iter. Eval: 0.0009 s/iter. Total: 1.5206 s/iter. ETA=0:08:14
[07/09 19:53:57] d2.evaluation.evaluator INFO: Inference done 179/500. Dataloading: 0.0022 s/iter. Inference: 1.5166 s/iter. Eval: 0.0009 s/iter. Total: 1.5198 s/iter. ETA=0:08:07
[07/09 19:54:03] d2.evaluation.evaluator INFO: Inference done 183/500. Dataloading: 0.0022 s/iter. Inference: 1.5151 s/iter. Eval: 0.0009 s/iter. Total: 1.5183 s/iter. ETA=0:08:01
[07/09 19:54:09] d2.evaluation.evaluator INFO: Inference done 187/500. Dataloading: 0.0022 s/iter. Inference: 1.5147 s/iter. Eval: 0.0008 s/iter. Total: 1.5178 s/iter. ETA=0:07:55
[07/09 19:54:15] d2.evaluation.evaluator INFO: Inference done 191/500. Dataloading: 0.0022 s/iter. Inference: 1.5151 s/iter. Eval: 0.0008 s/iter. Total: 1.5182 s/iter. ETA=0:07:49
[07/09 19:54:21] d2.evaluation.evaluator INFO: Inference done 195/500. Dataloading: 0.0022 s/iter. Inference: 1.5142 s/iter. Eval: 0.0008 s/iter. Total: 1.5174 s/iter. ETA=0:07:42
[07/09 19:54:27] d2.evaluation.evaluator INFO: Inference done 199/500. Dataloading: 0.0022 s/iter. Inference: 1.5139 s/iter. Eval: 0.0008 s/iter. Total: 1.5171 s/iter. ETA=0:07:36
[07/09 19:54:33] d2.evaluation.evaluator INFO: Inference done 203/500. Dataloading: 0.0022 s/iter. Inference: 1.5145 s/iter. Eval: 0.0009 s/iter. Total: 1.5177 s/iter. ETA=0:07:30
[07/09 19:54:39] d2.evaluation.evaluator INFO: Inference done 207/500. Dataloading: 0.0022 s/iter. Inference: 1.5148 s/iter. Eval: 0.0009 s/iter. Total: 1.5180 s/iter. ETA=0:07:24
[07/09 19:54:46] d2.evaluation.evaluator INFO: Inference done 211/500. Dataloading: 0.0022 s/iter. Inference: 1.5151 s/iter. Eval: 0.0008 s/iter. Total: 1.5183 s/iter. ETA=0:07:18
[07/09 19:54:52] d2.evaluation.evaluator INFO: Inference done 215/500. Dataloading: 0.0022 s/iter. Inference: 1.5154 s/iter. Eval: 0.0008 s/iter. Total: 1.5186 s/iter. ETA=0:07:12
[07/09 19:54:58] d2.evaluation.evaluator INFO: Inference done 219/500. Dataloading: 0.0022 s/iter. Inference: 1.5157 s/iter. Eval: 0.0008 s/iter. Total: 1.5188 s/iter. ETA=0:07:06
[07/09 19:55:04] d2.evaluation.evaluator INFO: Inference done 223/500. Dataloading: 0.0022 s/iter. Inference: 1.5164 s/iter. Eval: 0.0008 s/iter. Total: 1.5195 s/iter. ETA=0:07:00
[07/09 19:55:10] d2.evaluation.evaluator INFO: Inference done 227/500. Dataloading: 0.0022 s/iter. Inference: 1.5168 s/iter. Eval: 0.0008 s/iter. Total: 1.5200 s/iter. ETA=0:06:54
[07/09 19:55:16] d2.evaluation.evaluator INFO: Inference done 231/500. Dataloading: 0.0023 s/iter. Inference: 1.5172 s/iter. Eval: 0.0008 s/iter. Total: 1.5203 s/iter. ETA=0:06:48
[07/09 19:55:22] d2.evaluation.evaluator INFO: Inference done 235/500. Dataloading: 0.0023 s/iter. Inference: 1.5170 s/iter. Eval: 0.0008 s/iter. Total: 1.5201 s/iter. ETA=0:06:42
[07/09 19:55:28] d2.evaluation.evaluator INFO: Inference done 239/500. Dataloading: 0.0023 s/iter. Inference: 1.5161 s/iter. Eval: 0.0008 s/iter. Total: 1.5192 s/iter. ETA=0:06:36
[07/09 19:55:34] d2.evaluation.evaluator INFO: Inference done 243/500. Dataloading: 0.0023 s/iter. Inference: 1.5153 s/iter. Eval: 0.0008 s/iter. Total: 1.5185 s/iter. ETA=0:06:30
[07/09 19:55:40] d2.evaluation.evaluator INFO: Inference done 247/500. Dataloading: 0.0023 s/iter. Inference: 1.5146 s/iter. Eval: 0.0008 s/iter. Total: 1.5178 s/iter. ETA=0:06:23
[07/09 19:55:46] d2.evaluation.evaluator INFO: Inference done 251/500. Dataloading: 0.0023 s/iter. Inference: 1.5137 s/iter. Eval: 0.0008 s/iter. Total: 1.5169 s/iter. ETA=0:06:17
[07/09 19:55:52] d2.evaluation.evaluator INFO: Inference done 255/500. Dataloading: 0.0023 s/iter. Inference: 1.5133 s/iter. Eval: 0.0008 s/iter. Total: 1.5164 s/iter. ETA=0:06:11
[07/09 19:55:58] d2.evaluation.evaluator INFO: Inference done 259/500. Dataloading: 0.0023 s/iter. Inference: 1.5125 s/iter. Eval: 0.0008 s/iter. Total: 1.5157 s/iter. ETA=0:06:05
[07/09 19:56:04] d2.evaluation.evaluator INFO: Inference done 263/500. Dataloading: 0.0023 s/iter. Inference: 1.5116 s/iter. Eval: 0.0008 s/iter. Total: 1.5148 s/iter. ETA=0:05:58
[07/09 19:56:10] d2.evaluation.evaluator INFO: Inference done 267/500. Dataloading: 0.0023 s/iter. Inference: 1.5115 s/iter. Eval: 0.0008 s/iter. Total: 1.5146 s/iter. ETA=0:05:52
[07/09 19:56:15] d2.evaluation.evaluator INFO: Inference done 271/500. Dataloading: 0.0023 s/iter. Inference: 1.5107 s/iter. Eval: 0.0008 s/iter. Total: 1.5139 s/iter. ETA=0:05:46
[07/09 19:56:21] d2.evaluation.evaluator INFO: Inference done 275/500. Dataloading: 0.0023 s/iter. Inference: 1.5102 s/iter. Eval: 0.0008 s/iter. Total: 1.5133 s/iter. ETA=0:05:40
[07/09 19:56:27] d2.evaluation.evaluator INFO: Inference done 279/500. Dataloading: 0.0023 s/iter. Inference: 1.5101 s/iter. Eval: 0.0008 s/iter. Total: 1.5133 s/iter. ETA=0:05:34
[07/09 19:56:33] d2.evaluation.evaluator INFO: Inference done 283/500. Dataloading: 0.0023 s/iter. Inference: 1.5101 s/iter. Eval: 0.0008 s/iter. Total: 1.5132 s/iter. ETA=0:05:28
[07/09 19:56:39] d2.evaluation.evaluator INFO: Inference done 287/500. Dataloading: 0.0023 s/iter. Inference: 1.5095 s/iter. Eval: 0.0008 s/iter. Total: 1.5127 s/iter. ETA=0:05:22
[07/09 19:56:45] d2.evaluation.evaluator INFO: Inference done 291/500. Dataloading: 0.0023 s/iter. Inference: 1.5089 s/iter. Eval: 0.0008 s/iter. Total: 1.5120 s/iter. ETA=0:05:16
[07/09 19:56:51] d2.evaluation.evaluator INFO: Inference done 295/500. Dataloading: 0.0023 s/iter. Inference: 1.5082 s/iter. Eval: 0.0008 s/iter. Total: 1.5113 s/iter. ETA=0:05:09
[07/09 19:56:57] d2.evaluation.evaluator INFO: Inference done 299/500. Dataloading: 0.0023 s/iter. Inference: 1.5077 s/iter. Eval: 0.0008 s/iter. Total: 1.5108 s/iter. ETA=0:05:03
[07/09 19:57:03] d2.evaluation.evaluator INFO: Inference done 303/500. Dataloading: 0.0023 s/iter. Inference: 1.5070 s/iter. Eval: 0.0008 s/iter. Total: 1.5102 s/iter. ETA=0:04:57
[07/09 19:57:09] d2.evaluation.evaluator INFO: Inference done 307/500. Dataloading: 0.0023 s/iter. Inference: 1.5066 s/iter. Eval: 0.0008 s/iter. Total: 1.5097 s/iter. ETA=0:04:51
[07/09 19:57:15] d2.evaluation.evaluator INFO: Inference done 311/500. Dataloading: 0.0023 s/iter. Inference: 1.5063 s/iter. Eval: 0.0008 s/iter. Total: 1.5094 s/iter. ETA=0:04:45
[07/09 19:57:21] d2.evaluation.evaluator INFO: Inference done 315/500. Dataloading: 0.0023 s/iter. Inference: 1.5063 s/iter. Eval: 0.0008 s/iter. Total: 1.5094 s/iter. ETA=0:04:39
[07/09 19:57:27] d2.evaluation.evaluator INFO: Inference done 319/500. Dataloading: 0.0023 s/iter. Inference: 1.5065 s/iter. Eval: 0.0008 s/iter. Total: 1.5096 s/iter. ETA=0:04:33
[07/09 19:57:33] d2.evaluation.evaluator INFO: Inference done 323/500. Dataloading: 0.0023 s/iter. Inference: 1.5077 s/iter. Eval: 0.0008 s/iter. Total: 1.5109 s/iter. ETA=0:04:27
[07/09 19:57:40] d2.evaluation.evaluator INFO: Inference done 327/500. Dataloading: 0.0023 s/iter. Inference: 1.5086 s/iter. Eval: 0.0008 s/iter. Total: 1.5117 s/iter. ETA=0:04:21
[07/09 19:57:46] d2.evaluation.evaluator INFO: Inference done 331/500. Dataloading: 0.0023 s/iter. Inference: 1.5095 s/iter. Eval: 0.0008 s/iter. Total: 1.5126 s/iter. ETA=0:04:15
[07/09 19:57:52] d2.evaluation.evaluator INFO: Inference done 335/500. Dataloading: 0.0023 s/iter. Inference: 1.5103 s/iter. Eval: 0.0008 s/iter. Total: 1.5134 s/iter. ETA=0:04:09
[07/09 19:57:59] d2.evaluation.evaluator INFO: Inference done 339/500. Dataloading: 0.0023 s/iter. Inference: 1.5111 s/iter. Eval: 0.0008 s/iter. Total: 1.5143 s/iter. ETA=0:04:03
[07/09 19:58:05] d2.evaluation.evaluator INFO: Inference done 343/500. Dataloading: 0.0023 s/iter. Inference: 1.5119 s/iter. Eval: 0.0008 s/iter. Total: 1.5150 s/iter. ETA=0:03:57
[07/09 19:58:11] d2.evaluation.evaluator INFO: Inference done 347/500. Dataloading: 0.0023 s/iter. Inference: 1.5121 s/iter. Eval: 0.0008 s/iter. Total: 1.5152 s/iter. ETA=0:03:51
[07/09 19:58:17] d2.evaluation.evaluator INFO: Inference done 351/500. Dataloading: 0.0023 s/iter. Inference: 1.5117 s/iter. Eval: 0.0008 s/iter. Total: 1.5148 s/iter. ETA=0:03:45
[07/09 19:58:23] d2.evaluation.evaluator INFO: Inference done 355/500. Dataloading: 0.0023 s/iter. Inference: 1.5110 s/iter. Eval: 0.0008 s/iter. Total: 1.5141 s/iter. ETA=0:03:39
[07/09 19:58:29] d2.evaluation.evaluator INFO: Inference done 359/500. Dataloading: 0.0023 s/iter. Inference: 1.5105 s/iter. Eval: 0.0008 s/iter. Total: 1.5136 s/iter. ETA=0:03:33
[07/09 19:58:34] d2.evaluation.evaluator INFO: Inference done 363/500. Dataloading: 0.0023 s/iter. Inference: 1.5100 s/iter. Eval: 0.0008 s/iter. Total: 1.5131 s/iter. ETA=0:03:27
[07/09 19:58:40] d2.evaluation.evaluator INFO: Inference done 367/500. Dataloading: 0.0023 s/iter. Inference: 1.5095 s/iter. Eval: 0.0008 s/iter. Total: 1.5126 s/iter. ETA=0:03:21
[07/09 19:58:46] d2.evaluation.evaluator INFO: Inference done 371/500. Dataloading: 0.0023 s/iter. Inference: 1.5097 s/iter. Eval: 0.0008 s/iter. Total: 1.5128 s/iter. ETA=0:03:15
[07/09 19:58:52] d2.evaluation.evaluator INFO: Inference done 375/500. Dataloading: 0.0023 s/iter. Inference: 1.5094 s/iter. Eval: 0.0008 s/iter. Total: 1.5126 s/iter. ETA=0:03:09
[07/09 19:58:58] d2.evaluation.evaluator INFO: Inference done 379/500. Dataloading: 0.0023 s/iter. Inference: 1.5088 s/iter. Eval: 0.0008 s/iter. Total: 1.5120 s/iter. ETA=0:03:02
[07/09 19:59:04] d2.evaluation.evaluator INFO: Inference done 383/500. Dataloading: 0.0023 s/iter. Inference: 1.5088 s/iter. Eval: 0.0008 s/iter. Total: 1.5119 s/iter. ETA=0:02:56
[07/09 19:59:10] d2.evaluation.evaluator INFO: Inference done 387/500. Dataloading: 0.0023 s/iter. Inference: 1.5086 s/iter. Eval: 0.0008 s/iter. Total: 1.5117 s/iter. ETA=0:02:50
[07/09 19:59:16] d2.evaluation.evaluator INFO: Inference done 391/500. Dataloading: 0.0023 s/iter. Inference: 1.5084 s/iter. Eval: 0.0008 s/iter. Total: 1.5115 s/iter. ETA=0:02:44
[07/09 19:59:22] d2.evaluation.evaluator INFO: Inference done 395/500. Dataloading: 0.0023 s/iter. Inference: 1.5080 s/iter. Eval: 0.0008 s/iter. Total: 1.5111 s/iter. ETA=0:02:38
[07/09 19:59:28] d2.evaluation.evaluator INFO: Inference done 399/500. Dataloading: 0.0023 s/iter. Inference: 1.5082 s/iter. Eval: 0.0008 s/iter. Total: 1.5113 s/iter. ETA=0:02:32
[07/09 19:59:34] d2.evaluation.evaluator INFO: Inference done 403/500. Dataloading: 0.0023 s/iter. Inference: 1.5084 s/iter. Eval: 0.0008 s/iter. Total: 1.5115 s/iter. ETA=0:02:26
[07/09 19:59:40] d2.evaluation.evaluator INFO: Inference done 407/500. Dataloading: 0.0023 s/iter. Inference: 1.5086 s/iter. Eval: 0.0008 s/iter. Total: 1.5117 s/iter. ETA=0:02:20
[07/09 19:59:47] d2.evaluation.evaluator INFO: Inference done 411/500. Dataloading: 0.0023 s/iter. Inference: 1.5088 s/iter. Eval: 0.0008 s/iter. Total: 1.5119 s/iter. ETA=0:02:14
[07/09 19:59:53] d2.evaluation.evaluator INFO: Inference done 415/500. Dataloading: 0.0023 s/iter. Inference: 1.5090 s/iter. Eval: 0.0008 s/iter. Total: 1.5121 s/iter. ETA=0:02:08
[07/09 19:59:59] d2.evaluation.evaluator INFO: Inference done 419/500. Dataloading: 0.0023 s/iter. Inference: 1.5091 s/iter. Eval: 0.0008 s/iter. Total: 1.5122 s/iter. ETA=0:02:02
[07/09 20:00:05] d2.evaluation.evaluator INFO: Inference done 423/500. Dataloading: 0.0023 s/iter. Inference: 1.5092 s/iter. Eval: 0.0008 s/iter. Total: 1.5124 s/iter. ETA=0:01:56
[07/09 20:00:11] d2.evaluation.evaluator INFO: Inference done 427/500. Dataloading: 0.0023 s/iter. Inference: 1.5093 s/iter. Eval: 0.0008 s/iter. Total: 1.5125 s/iter. ETA=0:01:50
[07/09 20:00:17] d2.evaluation.evaluator INFO: Inference done 431/500. Dataloading: 0.0023 s/iter. Inference: 1.5094 s/iter. Eval: 0.0008 s/iter. Total: 1.5126 s/iter. ETA=0:01:44
[07/09 20:00:23] d2.evaluation.evaluator INFO: Inference done 435/500. Dataloading: 0.0023 s/iter. Inference: 1.5095 s/iter. Eval: 0.0008 s/iter. Total: 1.5127 s/iter. ETA=0:01:38
[07/09 20:00:29] d2.evaluation.evaluator INFO: Inference done 439/500. Dataloading: 0.0023 s/iter. Inference: 1.5098 s/iter. Eval: 0.0008 s/iter. Total: 1.5129 s/iter. ETA=0:01:32
[07/09 20:00:35] d2.evaluation.evaluator INFO: Inference done 443/500. Dataloading: 0.0023 s/iter. Inference: 1.5094 s/iter. Eval: 0.0008 s/iter. Total: 1.5125 s/iter. ETA=0:01:26
[07/09 20:00:41] d2.evaluation.evaluator INFO: Inference done 447/500. Dataloading: 0.0023 s/iter. Inference: 1.5091 s/iter. Eval: 0.0008 s/iter. Total: 1.5122 s/iter. ETA=0:01:20
[07/09 20:00:47] d2.evaluation.evaluator INFO: Inference done 451/500. Dataloading: 0.0023 s/iter. Inference: 1.5087 s/iter. Eval: 0.0008 s/iter. Total: 1.5118 s/iter. ETA=0:01:14
[07/09 20:00:53] d2.evaluation.evaluator INFO: Inference done 455/500. Dataloading: 0.0023 s/iter. Inference: 1.5082 s/iter. Eval: 0.0008 s/iter. Total: 1.5114 s/iter. ETA=0:01:08
[07/09 20:00:59] d2.evaluation.evaluator INFO: Inference done 459/500. Dataloading: 0.0023 s/iter. Inference: 1.5079 s/iter. Eval: 0.0008 s/iter. Total: 1.5110 s/iter. ETA=0:01:01
[07/09 20:01:05] d2.evaluation.evaluator INFO: Inference done 463/500. Dataloading: 0.0023 s/iter. Inference: 1.5076 s/iter. Eval: 0.0008 s/iter. Total: 1.5107 s/iter. ETA=0:00:55
[07/09 20:01:11] d2.evaluation.evaluator INFO: Inference done 467/500. Dataloading: 0.0023 s/iter. Inference: 1.5073 s/iter. Eval: 0.0008 s/iter. Total: 1.5104 s/iter. ETA=0:00:49
[07/09 20:01:17] d2.evaluation.evaluator INFO: Inference done 471/500. Dataloading: 0.0023 s/iter. Inference: 1.5076 s/iter. Eval: 0.0008 s/iter. Total: 1.5107 s/iter. ETA=0:00:43
[07/09 20:01:23] d2.evaluation.evaluator INFO: Inference done 475/500. Dataloading: 0.0023 s/iter. Inference: 1.5076 s/iter. Eval: 0.0008 s/iter. Total: 1.5108 s/iter. ETA=0:00:37
[07/09 20:01:29] d2.evaluation.evaluator INFO: Inference done 479/500. Dataloading: 0.0023 s/iter. Inference: 1.5083 s/iter. Eval: 0.0008 s/iter. Total: 1.5115 s/iter. ETA=0:00:31
[07/09 20:01:36] d2.evaluation.evaluator INFO: Inference done 483/500. Dataloading: 0.0023 s/iter. Inference: 1.5091 s/iter. Eval: 0.0008 s/iter. Total: 1.5122 s/iter. ETA=0:00:25
[07/09 20:01:42] d2.evaluation.evaluator INFO: Inference done 487/500. Dataloading: 0.0023 s/iter. Inference: 1.5097 s/iter. Eval: 0.0008 s/iter. Total: 1.5129 s/iter. ETA=0:00:19
[07/09 20:01:48] d2.evaluation.evaluator INFO: Inference done 491/500. Dataloading: 0.0023 s/iter. Inference: 1.5104 s/iter. Eval: 0.0008 s/iter. Total: 1.5135 s/iter. ETA=0:00:13
[07/09 20:01:55] d2.evaluation.evaluator INFO: Inference done 495/500. Dataloading: 0.0023 s/iter. Inference: 1.5110 s/iter. Eval: 0.0008 s/iter. Total: 1.5142 s/iter. ETA=0:00:07
[07/09 20:02:01] d2.evaluation.evaluator INFO: Inference done 499/500. Dataloading: 0.0023 s/iter. Inference: 1.5117 s/iter. Eval: 0.0008 s/iter. Total: 1.5148 s/iter. ETA=0:00:01
[07/09 20:02:03] d2.evaluation.evaluator INFO: Total inference time: 0:12:30.068447 (1.515290 s / iter per device, on 1 devices)
[07/09 20:02:03] d2.evaluation.evaluator INFO: Total inference pure compute time: 0:12:28 (1.511828 s / iter per device, on 1 devices)
[07/09 20:02:03] adet.evaluation.text_evaluation_all INFO: Saving results to output/vitaev2_s/150k_tt_mlt_13_15_textocr/finetune/ic15/inference/text_results.json
[07/09 20:02:13] d2.engine.defaults INFO: Evaluation results for ic15_test in csv format:
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: Task: DETECTION_ONLY_RESULTS
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: 0.6115,0.7814,0.6861
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: Task: None-E2E_RESULTS
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: 0.4706,0.6013,0.5280
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: Task: Strong-E2E_RESULTS
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: precision,recall,hmean
[07/09 20:02:13] d2.evaluation.testing INFO: copypaste: 0.7674,0.7116,0.7384
